{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RubberLanding/AdversarialMachineLearning24/blob/nico/adversarial_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWRfFtr2j9T-",
        "outputId": "65d7a376-da31-4bd7-a297-e47767a61c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "# !pip install git+https://github.com/fra31/auto-attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgFph_IRo8BW",
        "outputId": "d5b6e08f-3c98-4337-b078-204e0f122b1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=17fmN8eQdLpq2jIMQ_X0IXDPXfI9oVWgq\n",
            "From (redirected): https://drive.google.com/uc?id=17fmN8eQdLpq2jIMQ_X0IXDPXfI9oVWgq&confirm=t&uuid=ccb31ef0-0748-4558-8175-1e76db1fcb31\n",
            "To: /content/state_dicts.zip\n",
            "100% 979M/979M [00:07<00:00, 125MB/s]\n"
          ]
        }
      ],
      "source": [
        "# \"\"\"Download pre-trained weights for ResNet18 on CIFAR10\"\"\"\n",
        "# !pip install gdown\n",
        "\n",
        "# # Source: https://github.com/huyvnphan/PyTorch_CIFAR10\n",
        "# FILE_ID = \"17fmN8eQdLpq2jIMQ_X0IXDPXfI9oVWgq\"\n",
        "# file_url = f\"https://drive.google.com/uc?id={FILE_ID}\"\n",
        "\n",
        "# !gdown {file_url}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DIOCbEGHlAu9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.optim import SGD, Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX34q7fo-l5R",
        "outputId": "a024c9af-6170-454d-987c-7f0143b3fc57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "project_dir = Path('/content/drive/MyDrive/adversarial_training')\n",
        "project_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "weight_dir = project_dir / \"weights\"\n",
        "weight_dir.mkdir(parents=True, exist_ok=True)\n",
        "weight_file = weight_dir / \"resnet18.pt\"\n",
        "\n",
        "# \"\"\"Extract the pre-trained model weights to Google Drive\"\"\"\n",
        "# with zipfile.ZipFile(\"state_dicts.zip\", \"r\") as zip_ref:\n",
        "#   # print(zip_ref.namelist())\n",
        "#   with zip_ref.open(\"state_dicts/resnet18.pt\") as zf, open(weight_file, 'wb') as f:\n",
        "#       shutil.copyfileobj(zf, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bqVEbYQFJS_c"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "def generate_run_name():\n",
        "    \"\"\"Generate a random name for a run.\"\"\"\n",
        "    colors = [\n",
        "        \"red\", \"blue\", \"green\", \"yellow\", \"purple\", \"orange\", \"pink\",\n",
        "        \"black\", \"white\", \"gray\", \"silver\", \"gold\", \"cyan\", \"magenta\"]\n",
        "    adjectives = [\n",
        "        \"fast\", \"slow\", \"shiny\", \"dull\", \"bright\", \"dark\", \"silent\",\n",
        "        \"loud\", \"brave\", \"calm\", \"wise\", \"fierce\", \"kind\", \"strong\"]\n",
        "    nouns = [\n",
        "        \"dragon\", \"tiger\", \"lion\", \"panda\", \"wolf\", \"phoenix\", \"eagle\",\n",
        "        \"fox\", \"bear\", \"shark\", \"hawk\", \"cheetah\", \"whale\", \"octopus\"]\n",
        "    color = random.choice(colors)\n",
        "    adjective = random.choice(adjectives)\n",
        "    noun = random.choice(nouns)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "    run_name = f\"{color}-{adjective}-{noun}-{timestamp}\"\n",
        "    return run_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLo5cWwgkquv",
        "outputId": "46cf844c-b474-487f-9c98-672a9397f0ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to datasets/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 30.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/cifar-10-python.tar.gz to datasets\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "batch_size = 1024 # batch size has to be < 2**16, should be <= 2**13 for T4\n",
        "debug = True\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.491, 0.482, 0.446], std=[0.247, 0.243, 0.261]),\n",
        "    ])\n",
        "\n",
        "\"\"\" Load data \"\"\"\n",
        "data_train = CIFAR10(root=\"datasets\", train=True, download=True, transform=transform)\n",
        "data_test = CIFAR10(root=\"datasets\", train=False, download=True, transform=transform)\n",
        "num_classes = len(data_train.classes)\n",
        "\n",
        "# mean = data_train.data.mean(axis=(0,1,2)) / 255 # [0.49139968, 0.48215841, 0.44653091]\n",
        "# std = data_train.data.std(axis=(0,1,2)) / 255 # [0.24703223, 0.24348513, 0.26158784]\n",
        "\n",
        "data_train_subset = Subset(data_train, list(range(2*batch_size)))\n",
        "data_test_subset = Subset(data_test, list(range(2*batch_size)))\n",
        "\n",
        "dataloader_train = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
        "dataloader_test = DataLoader(data_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GSHC7WdoyxSC"
      },
      "outputs": [],
      "source": [
        "def fgsm(model, X, y, epsilon=8/255):\n",
        "    \"\"\" Construct FGSM adversarial examples on the examples X\"\"\"\n",
        "    delta = torch.zeros_like(X, requires_grad=True)\n",
        "    loss = CrossEntropyLoss()(model(X + delta), y)\n",
        "    loss.backward()\n",
        "    return epsilon * delta.grad.detach().sign()\n",
        "\n",
        "def pgd_linf(model, X, y, epsilon=16/255, alpha=2/255, num_iter=10, randomize=False):\n",
        "    \"\"\" Construct FGSM adversarial examples on the examples X\"\"\"\n",
        "    if randomize:\n",
        "        delta = torch.rand_like(X, requires_grad=True)\n",
        "        delta.data = delta.data * 2 * epsilon - epsilon\n",
        "    else:\n",
        "        delta = torch.zeros_like(X, requires_grad=True)\n",
        "\n",
        "    for t in range(num_iter):\n",
        "        loss = CrossEntropyLoss()(model(X + delta), y)\n",
        "        loss.backward()\n",
        "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
        "        delta.grad.zero_()\n",
        "    return delta.detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Dxm_8JdYoK3D"
      },
      "outputs": [],
      "source": [
        "def train_epoch(loader, model, opt):\n",
        "    \"\"\"Standard training/evaluation epoch over the dataset\"\"\"\n",
        "    model.train()\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        yp = model(X)\n",
        "        loss = CrossEntropyLoss()(yp,y)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(loader, model):\n",
        "    \"\"\"Standard training/evaluation epoch over the dataset\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        yp = model(X)\n",
        "        loss = CrossEntropyLoss()(yp,y)\n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n",
        "def train_epoch_adversarial(loader, model, attack, opt, **kwargs):\n",
        "    \"\"\"Adversarial training/evaluation epoch over the dataset\"\"\"\n",
        "    model.train()\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        delta = attack(model, X, y, **kwargs)\n",
        "        yp = model(X+delta)\n",
        "        loss = CrossEntropyLoss()(yp,y)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n",
        "def eval_epoch_adversarial(loader, model, attack, **kwargs):\n",
        "    \"\"\"Adversarial training/evaluation epoch over the dataset\"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss, total_err = 0.0, 0.0\n",
        "\n",
        "    for X, y in loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute adversarial perturbations (requires gradients)\n",
        "        with torch.enable_grad():\n",
        "            delta = attack(model, X, y, **kwargs)\n",
        "\n",
        "        # Evaluate the model on adversarial examples without gradients\n",
        "        with torch.no_grad():\n",
        "            yp = model(X + delta)\n",
        "            loss = torch.nn.CrossEntropyLoss()(yp, y)\n",
        "\n",
        "            total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "            total_loss += loss.item() * X.shape[0]\n",
        "\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "LyCAOqCdy-Gs",
        "outputId": "9b8b6156-3eb6-4a8a-a28a-38a180258337"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fc51e7626ec9>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpretrained_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m                         return _load(\n\u001b[0m\u001b[1;32m   1352\u001b[0m                             \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m                             \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_weights_only_unpickler.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m                         \u001b[0;34mf\"Only persistent_load of storage is allowed, but got {pid[0]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                     )\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBINGET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLONG_BINGET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mBINGET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1810\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m             typed_storage = load_tensor(\n\u001b[0m\u001b[1;32m   1813\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m             storage = (\n\u001b[0;32m-> 1772\u001b[0;31m                 \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1773\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0m_untyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\"\"\" Regular Training \"\"\"\n",
        "import json\n",
        "from torch.nn import CrossEntropyLoss, Conv2d\n",
        "\n",
        "model_reg = resnet18()\n",
        "\n",
        "# CIFAR10: kernel_size 7 -> 3, stride 2 -> 1, padding 3->1\n",
        "model_reg.conv1 = Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_reg.fc = torch.nn.Linear(model_reg.fc.in_features, num_classes)\n",
        "\n",
        "pretrained_weights = torch.load(weight_file, weights_only=True)\n",
        "model_reg.load_state_dict(pretrained_weights)\n",
        "model_reg = model_reg.to(device)\n",
        "\n",
        "run_dir = project_dir / generate_run_name()\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "opt = SGD(model_reg.parameters(), lr=1e-1)\n",
        "opt = Adam(model_reg.parameters(), lr=1e-3)\n",
        "\n",
        "epochs = 2\n",
        "log = {key: [] for key in [\"train_losses\", \"test_losses\", \"adv_losses\",\n",
        "                           \"train_errors\", \"test_errors\", \"adv_errors\"]}\n",
        "\n",
        "print(f\"Begin adversarial training run: {run_dir.stem}\\n\")\n",
        "print(*(\"TR      \", \"TE      \", \"ADV     \", \"     \"), sep=\"\\t\")\n",
        "\n",
        "for t in range(epochs):\n",
        "    train_err, train_loss = train_epoch(dataloader_train, model_reg, opt)\n",
        "    test_err, test_loss = eval_epoch(dataloader_test, model_reg)\n",
        "    adv_err, adv_loss = eval_epoch_adversarial(dataloader_test, model_reg, fgsm)\n",
        "\n",
        "    # Update the losses and errors\n",
        "    log[\"train_losses\"] += [train_loss]\n",
        "    log[\"test_losses\"] += [test_loss]\n",
        "    log[\"adv_losses\"] += [adv_loss]\n",
        "    log[\"train_errors\"] += [train_err]\n",
        "    log[\"test_errors\"] += [test_err]\n",
        "    log[\"adv_errors\"] += [adv_err]\n",
        "\n",
        "    print(*(\"{:.6f}\".format(train_err),\n",
        "            \"{:.6f}\".format(test_err),\n",
        "            \"{:.6f}\".format(adv_err),\n",
        "            f\"Epoch: {t+1}\",), sep=\"\\t\")\n",
        "\n",
        "with open(run_dir / \"log.json\", \"w\") as f:\n",
        "    json.dump(log, f)\n",
        "torch.save(model_reg.state_dict(), run_dir / \"model_reg.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNAZgohWXLfM",
        "outputId": "a56be6dc-21df-4760-a502-524aa7fc3a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin adversarial training run: silver-kind-dragon-20250105-1328\n",
            "\n",
            "TR      \tTE      \tADV     \tEpoch   \n"
          ]
        }
      ],
      "source": [
        "\"\"\" Adversarial Training \"\"\"\n",
        "import json\n",
        "from torch.nn import CrossEntropyLoss, Conv2d\n",
        "\n",
        "model_adv = resnet18()\n",
        "\n",
        "# CIFAR10: kernel_size 7 -> 3, stride 2 -> 1, padding 3->1\n",
        "model_adv.conv1 = Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_adv.fc = torch.nn.Linear(model_adv.fc.in_features, num_classes)\n",
        "\n",
        "pretrained_weights = torch.load(weight_file, weights_only=True)\n",
        "model_adv.load_state_dict(pretrained_weights)\n",
        "model_adv = model_adv.to(device)\n",
        "\n",
        "run_dir = project_dir / generate_run_name()\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# opt = SGD(model_adv.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
        "opt = Adam(model_adv.parameters(), lr=1e-3)\n",
        "# opt = SGD(model_adv.parameters(), lr=1e-1)\n",
        "# opt = SGD(model_adv.parameters(), lr=1e-1, weight_decay=5e-4)\n",
        "\n",
        "# scheduler = CosineAnnealingLR(opt, T_max=100)\n",
        "# scheduler = CosineAnnealingWarmRestarts(opt, T_0=10, T_mult=2, eta_min=0)\n",
        "\n",
        "epochs = 5\n",
        "log = {key: [] for key in [\"train_losses\", \"test_losses\", \"adv_losses\",\n",
        "                           \"train_errors\", \"test_errors\", \"adv_errors\"]}\n",
        "\n",
        "print(f\"Begin adversarial training run: {run_dir.stem}\\n\")\n",
        "print(*(\"TR      \", \"TE      \", \"ADV     \", \"Epoch   \"), sep=\"\\t\")\n",
        "for t in range(epochs):\n",
        "    train_err, train_loss = train_epoch_adversarial(dataloader_train, model_adv, pgd_linf, opt)\n",
        "    test_err, test_loss = eval_epoch(dataloader_test, model_adv)\n",
        "    adv_err, adv_loss = eval_epoch_adversarial(dataloader_test, model_adv, fgsm)\n",
        "\n",
        "    # Update the losses and errors\n",
        "    log[\"train_losses\"] += [train_loss]\n",
        "    log[\"test_losses\"] += [test_loss]\n",
        "    log[\"adv_losses\"] += [adv_loss]\n",
        "    log[\"train_errors\"] += [train_err]\n",
        "    log[\"test_errors\"] += [test_err]\n",
        "    log[\"adv_errors\"] += [adv_err]\n",
        "\n",
        "    print(*(\"{:.6f}\".format(train_err),\n",
        "            \"{:.6f}\".format(test_err),\n",
        "            \"{:.6f}\".format(adv_err),\n",
        "            f\"{t+1}\",), sep=\"\\t\")\n",
        "\n",
        "with open(run_dir / \"log.json\", \"w\") as f:\n",
        "    json.dump(log, f)\n",
        "torch.save(model_adv.state_dict(), run_dir / \"model_adv.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcUht5uBHlYG"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "gold-bright-eagle-20241204-1128:\n",
        "SGD(lr=1e-1), batch_size=1024\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "orange-kind-hawk-20241204-1148:\n",
        "SGD(lr=1e-1, weight_decay=5e-4), batch_size=1024\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "silver-shiny-phoenix-20241204-1210:\n",
        "SGD(lr=1e-1, weight_decay=5e-4, momentum=0.9, nesterov=True), batch_size=1024\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "gray-fierce-octopus-20241204-1226:\n",
        "SGD(lr=1e-1, weight_decay=5e-4, momentum=0.9, nesterov=True), batch_size=1024, CosineAnnealingLR\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "gold-loud-cheetah-20241204-1414:\n",
        "SGD(lr=1e-1, weight_decay=5e-4, momentum=0.9, nesterov=True), batch_size=1024,, epsilon=4/255\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "cyan-wise-eagle-20241203-1710:\n",
        "SGD(lr=1e-1, weight_decay=5e-4, momentum=0.9, nesterov=True), batch_size=1024,, epsilon=32/255\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
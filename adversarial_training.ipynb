{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RubberLanding/AdversarialMachineLearning24/blob/evaluation/adversarial_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preliminaries\n"
      ],
      "metadata": {
        "id": "JN4iX5SEZ1Aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install all needed packages."
      ],
      "metadata": {
        "id": "-bTdL_l2Z6LQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWRfFtr2j9T-",
        "outputId": "d48ff5a2-0c3e-48c8-ddd6-3c7adb741775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "# !pip install git+https://github.com/fra31/auto-attack\n",
        "# !git pull https://github.com/RubberLanding/AdversarialMachineLearning24"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the weights for the pre-trained Resnet18 on CIFAR-10. We only do this once and store them on a private GoogleDrive, which we later import to be able to actually load the weights."
      ],
      "metadata": {
        "id": "3BunrBffT8NG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KgFph_IRo8BW"
      },
      "outputs": [],
      "source": [
        "# \"\"\"Download pre-trained weights for ResNet18 on CIFAR10\"\"\"\n",
        "# !pip install gdown\n",
        "\n",
        "# # Source: https://github.com/huyvnphan/PyTorch_CIFAR10\n",
        "# FILE_ID = \"17fmN8eQdLpq2jIMQ_X0IXDPXfI9oVWgq\"\n",
        "# file_url = f\"https://drive.google.com/uc?id={FILE_ID}\"\n",
        "\n",
        "# !gdown {file_url}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all packages und functions we need."
      ],
      "metadata": {
        "id": "z95qM1xeZ-g2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DIOCbEGHlAu9",
        "outputId": "6a6b246b-88cb-4893-9f13-1065d84dac8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Only a single TORCH_LIBRARY can be used to register the namespace prims; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at /dev/null:241; latest registration was registered at /dev/null:241",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6e2664d7456f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresnet18\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mToTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodulefinder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Don't re-order these, we need to load the _C extension (done when importing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSymBool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from torch._decomp import (\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0m_add_op_to_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0m_convert_out_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;31m# populate the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_decomp/decompositions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_prims/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mprim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prims\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DEF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mprim_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prims\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IMPL\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CompositeExplicitAutograd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprim_backend_select_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prims\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IMPL\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"BackendSelect\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/library.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ns, kind, dispatch_key)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         self.m: Optional[Any] = torch._C._dispatch_library(\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdispatch_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Only a single TORCH_LIBRARY can be used to register the namespace prims; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at /dev/null:241; latest registration was registered at /dev/null:241"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.optim import SGD, Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount your GoogleDrive where you stored the weights for the pre-trained Resnet18 before. We will also store the logging files and model weights there."
      ],
      "metadata": {
        "id": "8KJEoOzHUIjv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX34q7fo-l5R"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "project_dir = Path('/content/drive/MyDrive/adversarial_training')\n",
        "project_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "weight_dir = project_dir / \"weights\"\n",
        "weight_dir.mkdir(parents=True, exist_ok=True)\n",
        "weight_file = weight_dir / \"resnet18.pt\"\n",
        "\n",
        "# \"\"\"Extract the pre-trained model weights to Google Drive\"\"\"\n",
        "# with zipfile.ZipFile(\"state_dicts.zip\", \"r\") as zip_ref:\n",
        "#   # print(zip_ref.namelist())\n",
        "#   with zip_ref.open(\"state_dicts/resnet18.pt\") as zf, open(weight_file, 'wb') as f:\n",
        "#       shutil.copyfileobj(zf, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a convenience function to name the different training runs."
      ],
      "metadata": {
        "id": "DYM2QScUaE2z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqVEbYQFJS_c"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "def generate_run_name():\n",
        "    \"\"\"Generate a random name for a run.\"\"\"\n",
        "    colors = [\n",
        "        \"red\", \"blue\", \"green\", \"yellow\", \"purple\", \"orange\", \"pink\",\n",
        "        \"black\", \"white\", \"gray\", \"silver\", \"gold\", \"cyan\", \"magenta\"]\n",
        "    adjectives = [\n",
        "        \"fast\", \"slow\", \"shiny\", \"dull\", \"bright\", \"dark\", \"silent\",\n",
        "        \"loud\", \"brave\", \"calm\", \"wise\", \"fierce\", \"kind\", \"strong\"]\n",
        "    nouns = [\n",
        "        \"dragon\", \"tiger\", \"lion\", \"panda\", \"wolf\", \"phoenix\", \"eagle\",\n",
        "        \"fox\", \"bear\", \"shark\", \"hawk\", \"cheetah\", \"whale\", \"octopus\"]\n",
        "    color = random.choice(colors)\n",
        "    adjective = random.choice(adjectives)\n",
        "    noun = random.choice(nouns)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "    run_name = f\"{color}-{adjective}-{noun}-{timestamp}\"\n",
        "    return run_name"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "Download the CIFAR-10 data. Split it into training, validation and test set. Do some pre-processing."
      ],
      "metadata": {
        "id": "HgXUQL_saQG7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLo5cWwgkquv"
      },
      "outputs": [],
      "source": [
        "batch_size = 1024 # batch size has to be < 2**16, should be <= 2**13 for T4\n",
        "debug = True\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.491, 0.482, 0.446], std=[0.247, 0.243, 0.261]),\n",
        "    ])\n",
        "\n",
        "\"\"\" Load data \"\"\"\n",
        "data_train = CIFAR10(root=\"datasets\", train=True, download=True, transform=transform)\n",
        "data_test = CIFAR10(root=\"datasets\", train=False, download=True, transform=transform)\n",
        "data_test, data_val = torch.utils.data.random_split(data_test, [0.1, 0.9])\n",
        "\n",
        "dataloader_train = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
        "dataloader_test = DataLoader(data_test, batch_size=min(batch_size, len(data_test)), shuffle=False) # create test dataloader with a single batch\n",
        "dataloader_val = DataLoader(data_val, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "num_classes = len(data_train.classes)\n",
        "\n",
        "# mean = data_train.data.mean(axis=(0,1,2)) / 255 # [0.49139968, 0.48215841, 0.44653091]\n",
        "# std = data_train.data.std(axis=(0,1,2)) / 255 # [0.24703223, 0.24348513, 0.26158784]\n",
        "\n",
        "data_train_subset = Subset(data_train, list(range(2*batch_size)))\n",
        "data_val_subset = Subset(data_val, list(range(batch_size)))\n",
        "data_test_subset = Subset(data_test, list(range(100)))\n",
        "\n",
        "dataloader_train_subset = DataLoader(data_train_subset, batch_size=batch_size, shuffle=True)\n",
        "dataloader_val_subset = DataLoader(data_val_subset, batch_size=batch_size, shuffle=False)\n",
        "dataloader_test_subset = DataLoader(data_test_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attacks\n",
        "Define the attacks."
      ],
      "metadata": {
        "id": "TLTLCMphadmb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSHC7WdoyxSC"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def fgsm(model, X, y, epsilon=8/255):\n",
        "    \"\"\" Construct FGSM adversarial examples on the examples X\"\"\"\n",
        "    delta = torch.zeros_like(X, requires_grad=True)\n",
        "    loss = CrossEntropyLoss()(model(X + delta), y)\n",
        "    loss.backward()\n",
        "    return epsilon * delta.grad.detach().sign()\n",
        "\n",
        "def pgd_linf(model, X, y, epsilon=8/255, alpha=2/255, num_iter=10, randomize=False):\n",
        "    \"\"\" Construct FGSM adversarial examples on the examples X\"\"\"\n",
        "    if randomize:\n",
        "        delta = torch.rand_like(X, requires_grad=True)\n",
        "        delta.data = delta.data * 2 * epsilon - epsilon\n",
        "    else:\n",
        "        delta = torch.zeros_like(X, requires_grad=True)\n",
        "\n",
        "    for t in range(num_iter):\n",
        "        loss = CrossEntropyLoss()(model(X + delta), y)\n",
        "        loss.backward()\n",
        "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
        "        delta.grad.zero_()\n",
        "    return delta.detach()\n",
        "\n",
        "def pgd_linf_trades(model, X, y, epsilon=8/255, alpha=2/255, num_iter=10, randomize=False):\n",
        "    \"\"\" Construct FGSM adversarial examples with KL-Divergence for TRADES.\n",
        "        Should be used together with traded_loss().\n",
        "    \"\"\"\n",
        "    if randomize:\n",
        "        delta = torch.rand_like(X, requires_grad=True)\n",
        "        delta.data = delta.data * 2 * epsilon - epsilon\n",
        "    else:\n",
        "        delta = torch.zeros_like(X, requires_grad=True)\n",
        "\n",
        "    for t in range(num_iter):\n",
        "        # maybe set log_target=True and pass F.log_softmax(model(X), dim=1),\n",
        "        # see the docs for more details\n",
        "        loss = F.kl_div(F.log_softmax(model(X + delta), dim=1),\n",
        "                        F.softmax(model(X), dim=1),\n",
        "                        reduction='batchmean')\n",
        "        loss.backward()\n",
        "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
        "        delta.grad.zero_()\n",
        "    return delta.detach()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Losses\n",
        "We define a wrapper, that calls the appropriate loss function with the correct arguments. Inside the wrapper, we implement the loss functions."
      ],
      "metadata": {
        "id": "3cPBOyOCkiLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LossWrapper:\n",
        "    def __init__(self, loss_fn, lambda_tradeoff=1.0):\n",
        "        self.loss_fn = loss_fn\n",
        "        self.lambda_tradeoff = lambda_tradeoff\n",
        "\n",
        "    def __call__(self, model, X, y, delta=0.0):\n",
        "        \"\"\"Args:\n",
        "              model: The model being trained.\n",
        "              X: Clean input data.\n",
        "              delta: Perturbations applied to X.\n",
        "              y: Ground-truth labels.\n",
        "        \"\"\"\n",
        "        # Cross-Entropy Loss\n",
        "        if self.loss_fn == \"CE\":\n",
        "            yp = model(X + delta)\n",
        "            return F.cross_entropy(yp, y)\n",
        "\n",
        "        # TRADES Loss\n",
        "        elif self.loss_fn == \"TRADES\":\n",
        "            yp_adv = model(X + delta)\n",
        "            yp_clean = model(X)\n",
        "            clean_loss = F.cross_entropy(yp_clean, y)\n",
        "            robust_loss = F.kl_div(F.log_softmax(yp_adv, dim=1),\n",
        "                            F.softmax(yp_clean, dim=1),\n",
        "                            reduction='batchmean')\n",
        "            return clean_loss + self.lambda_tradeoff * robust_loss\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported loss function\")"
      ],
      "metadata": {
        "id": "9XMbYBVLkkWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "rSjV2ajnb_Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define functions that train a model."
      ],
      "metadata": {
        "id": "K5-e2T8AcHZV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxm_8JdYoK3D"
      },
      "outputs": [],
      "source": [
        "def train_epoch(loader, model, opt, loss_fn):\n",
        "    \"\"\"Standard training/evaluation epoch over the dataset\"\"\"\n",
        "    model.train()\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        yp = model(X)\n",
        "        loss = loss_fn(model, X, y)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(loader, model, loss_fn):\n",
        "    \"\"\"Standard training/evaluation epoch over the dataset\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        yp = model(X)\n",
        "        loss = loss_fn(model, X, y)\n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n",
        "def train_epoch_adversarial(loader, model, attack, opt, loss_fn, **kwargs):\n",
        "    \"\"\"Adversarial training/evaluation epoch over the dataset\"\"\"\n",
        "    model.train()\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        delta = attack(model, X, y, **kwargs)\n",
        "        yp = model(X+delta)\n",
        "        loss = loss_fn(model, X, y, delta)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n",
        "def eval_epoch_adversarial(loader, model, attack, loss_fn, **kwargs):\n",
        "    \"\"\"Adversarial training/evaluation epoch over the dataset\"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss, total_err = 0.0, 0.0\n",
        "\n",
        "    for X, y in loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute adversarial perturbations (requires gradients)\n",
        "        with torch.enable_grad():\n",
        "            delta = attack(model, X, y, **kwargs)\n",
        "\n",
        "        # Evaluate the model on adversarial examples without gradients\n",
        "        with torch.no_grad():\n",
        "            yp = model(X + delta)\n",
        "            loss = loss_fn(model, X, y, delta)\n",
        "\n",
        "            total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "            total_loss += loss.item() * X.shape[0]\n",
        "\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "zqdzqrZxNRYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the Advarsarial Training Model\n"
      ],
      "metadata": {
        "id": "oRmrIIaWd4rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from torch.nn import CrossEntropyLoss, Conv2d\n",
        "\n",
        "###########################\n",
        "# LOAD THE MODEL\n",
        "\n",
        "model_adv = resnet18()\n",
        "\n",
        "# CIFAR10: kernel_size 7 -> 3, stride 2 -> 1, padding 3->1\n",
        "model_adv.conv1 = Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_adv.fc = torch.nn.Linear(model_adv.fc.in_features, num_classes)\n",
        "\n",
        "pretrained_weights = torch.load(weight_file, weights_only=True)\n",
        "model_adv.load_state_dict(pretrained_weights)\n",
        "model_adv = model_adv.to(device)\n",
        "\n",
        "###########################\n",
        "# SET LOGGING\n",
        "\n",
        "run_dir = project_dir / 'trades_loss'\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "log = {key: [] for key in [\"train_losses\", \"test_losses\", \"adv_losses\",\n",
        "                           \"train_errors\", \"test_errors\", \"adv_errors\"]}\n"
      ],
      "metadata": {
        "id": "hLdE6jITd8PA",
        "outputId": "5a7b8909-f375-40b0-d895-23f2a6e5be74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/adversarial_training/adversarial_training/model_adv.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-4a6e5b82339b>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel_adv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_adv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mpretrained_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmodel_adv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_adv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/adversarial_training/adversarial_training/model_adv.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Adversarial Training Model\n",
        "Use adversarial training to train the robust model."
      ],
      "metadata": {
        "id": "xh7mVivIZqA6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNAZgohWXLfM",
        "outputId": "54741649-bb4d-4e02-df94-4bb079e9be73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin adversarial training run: trades_loss\n",
            "\n",
            "TR      \tTE      \tADV     \tEpoch   \n",
            "0.553711\t0.903320\t0.919922\t1\n",
            "0.615723\t0.594727\t0.738281\t2\n",
            "0.503906\t0.490234\t0.655273\t3\n",
            "0.452148\t0.453125\t0.644531\t4\n",
            "0.410156\t0.411133\t0.599609\t5\n",
            "0.370605\t0.375977\t0.565430\t6\n",
            "0.324219\t0.358398\t0.554688\t7\n",
            "0.295410\t0.336914\t0.553711\t8\n",
            "0.249023\t0.333984\t0.534180\t9\n",
            "0.197266\t0.325195\t0.553711\t10\n",
            "0.183594\t0.326172\t0.556641\t11\n",
            "0.146973\t0.326172\t0.539062\t12\n",
            "0.116699\t0.313477\t0.547852\t13\n",
            "0.081543\t0.329102\t0.565430\t14\n",
            "0.064453\t0.307617\t0.543945\t15\n",
            "0.047852\t0.303711\t0.556641\t16\n",
            "0.029785\t0.333008\t0.553711\t17\n",
            "0.029297\t0.343750\t0.563477\t18\n",
            "0.023926\t0.313477\t0.531250\t19\n",
            "0.013184\t0.329102\t0.541016\t20\n",
            "0.021484\t0.328125\t0.538086\t21\n",
            "0.022461\t0.358398\t0.533203\t22\n",
            "0.031250\t0.324219\t0.516602\t23\n",
            "0.009766\t0.346680\t0.547852\t24\n",
            "0.024414\t0.324219\t0.531250\t25\n",
            "0.006836\t0.308594\t0.522461\t26\n",
            "0.010254\t0.310547\t0.516602\t27\n",
            "0.005371\t0.305664\t0.506836\t28\n",
            "0.008301\t0.310547\t0.507812\t29\n",
            "0.005371\t0.316406\t0.527344\t30\n"
          ]
        }
      ],
      "source": [
        "###########################\n",
        "# SET TRAINING PARAMETERS\n",
        "\n",
        "opt = Adam(model_adv.parameters(), lr=1e-3, weight_decay=4e-3)\n",
        "# opt = SGD(model_adv.parameters(), lr=1e-1)\n",
        "# scheduler = CosineAnnealingLR(opt, T_max=100)\n",
        "# scheduler = CosineAnnealingWarmRestarts(opt, T_0=10, T_mult=2, eta_min=0)\n",
        "\n",
        "epochs = 30\n",
        "\n",
        "trades = LossWrapper(\"TRADES\")\n",
        "cross_entropy = LossWrapper(\"CE\")\n",
        "###########################\n",
        "# START TRAINING\n",
        "\n",
        "print(f\"Begin adversarial training run: {run_dir.stem}\\n\")\n",
        "print(*(\"TR      \", \"TE      \", \"ADV     \", \"Epoch   \"), sep=\"\\t\")\n",
        "\n",
        "for t in range(epochs):\n",
        "    train_err, train_loss = train_epoch_adversarial(dataloader_train_subset, model_adv, pgd_linf_trades, opt, loss_fn=trades, randomize=True)\n",
        "    test_err, test_loss = eval_epoch(dataloader_val_subset, model_adv, loss_fn=cross_entropy)\n",
        "    adv_err, adv_loss = eval_epoch_adversarial(dataloader_val_subset, model_adv, fgsm, loss_fn=cross_entropy)\n",
        "\n",
        "    # Update the losses and errors\n",
        "    log[\"train_losses\"] += [train_loss]\n",
        "    log[\"test_losses\"] += [test_loss]\n",
        "    log[\"adv_losses\"] += [adv_loss]\n",
        "    log[\"train_errors\"] += [train_err]\n",
        "    log[\"test_errors\"] += [test_err]\n",
        "    log[\"adv_errors\"] += [adv_err]\n",
        "\n",
        "    print(*(\"{:.6f}\".format(train_err),\n",
        "            \"{:.6f}\".format(test_err),\n",
        "            \"{:.6f}\".format(adv_err),\n",
        "            f\"{t+1}\",), sep=\"\\t\")\n",
        "\n",
        "###########################\n",
        "# STORE RESULTS\n",
        "store = True # set this variable to True when you have runs that you want to save\n",
        "if store:\n",
        "  with open(run_dir / \"log.json\", \"w\") as f:\n",
        "      json.dump(log, f)\n",
        "  torch.save(model_adv.state_dict(), run_dir / \"model_adv.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install AutoAttack to calculate a robust accuracy for the model to get a fair comparison, e.g. with models from [RobustBench](https://robustbench.github.io/)."
      ],
      "metadata": {
        "id": "n0Y3Q_ZlZy3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/fra31/auto-attack"
      ],
      "metadata": {
        "id": "3CJs5EBFu26D",
        "outputId": "f490ac6f-7cdf-464d-a4cc-76b667af7d32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/fra31/auto-attack\n",
            "  Cloning https://github.com/fra31/auto-attack to /tmp/pip-req-build-_zt7toza\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/fra31/auto-attack /tmp/pip-req-build-_zt7toza\n",
            "  Resolved https://github.com/fra31/auto-attack to commit a39220048b3c9f2cca9a4d3a54604793c68eca7e\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: autoattack\n",
            "  Building wheel for autoattack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autoattack: filename=autoattack-0.1-py3-none-any.whl size=36229 sha256=51f95e92f74f6a9f98b12157b543f4699a61fee88c816529e8190e7543eb9d24\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7ll2ry5m/wheels/b7/17/95/4b16fafe1b84fdabd247eb5f01b41165bd98d4c82d64ab93d2\n",
            "Successfully built autoattack\n",
            "Installing collected packages: autoattack\n",
            "Successfully installed autoattack-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autoattack import AutoAttack\n",
        "\n",
        "X, y = next(iter(dataloader_test)) # optimally, the test loader has a single batch containing the entire test set\n",
        "X, y = X.to(device), y.to(device)\n",
        "\n",
        "adversary = AutoAttack(model_adv, norm='Linf', eps=8/255, version='standard')\n",
        "adv_examples = adversary.run_standard_evaluation(X, y, bs=batch_size) # this takes ~8min on T4 on a single test batch"
      ],
      "metadata": {
        "id": "uknpMFbi73zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the Weight Averaging Model\n",
        "Here we initialize an exponential moving average (EMA) model, based on the robust model."
      ],
      "metadata": {
        "id": "O32SQzimNaFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from torch.nn import CrossEntropyLoss, Conv2d\n",
        "\n",
        "###########################\n",
        "# LOAD THE MODEL\n",
        "\n",
        "model_adv = resnet18()\n",
        "\n",
        "# CIFAR10: kernel_size 7 -> 3, stride 2 -> 1, padding 3->1\n",
        "model_adv.conv1 = Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_adv.fc = torch.nn.Linear(model_adv.fc.in_features, num_classes)\n",
        "\n",
        "pretrained_weights = torch.load(weight_file, weights_only=True)\n",
        "model_adv.load_state_dict(pretrained_weights)\n",
        "\n",
        "model_ema = torch.optim.swa_utils.AveragedModel(model_adv, multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.9))\n",
        "\n",
        "model_adv = model_adv.to(device)\n",
        "model_ema = model_ema.to(device)\n",
        "\n",
        "###########################\n",
        "# SET LOGGING\n",
        "\n",
        "run_dir = project_dir / 'weight_averaging'\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "log = {key: [] for key in [\"train_losses\", \"test_losses\", \"adv_losses\",\n",
        "                           \"train_errors\", \"test_errors\", \"adv_errors\"]}\n"
      ],
      "metadata": {
        "id": "3Ed0KCUxMdQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FML825DiaHuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Weight Averaged Model"
      ],
      "metadata": {
        "id": "FEf0SlFQaN0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# SET TRAINING PARAMETERS\n",
        "\n",
        "opt = Adam(model_adv.parameters(), lr=1e-3, weight_decay=4e-3)\n",
        "# opt = SGD(model_adv.parameters(), lr=1e-1)\n",
        "# scheduler = CosineAnnealingLR(opt, T_max=100)\n",
        "# scheduler = CosineAnnealingWarmRestarts(opt, T_0=10, T_mult=2, eta_min=0)\n",
        "\n",
        "epochs = 30\n",
        "\n",
        "trades = LossWrapper(\"TRADES\")\n",
        "cross_entropy = LossWrapper(\"CE\")\n",
        "\n",
        "###########################\n",
        "# START TRAINING\n",
        "\n",
        "print(f\"Begin adversarial training run: {run_dir.stem}\\n\")\n",
        "print(*(\"TR      \", \"TE      \", \"ADV     \", \"Epoch   \"), sep=\"\\t\")\n",
        "\n",
        "for t in range(epochs):\n",
        "    train_err, train_loss = train_epoch_adversarial(dataloader_train_subset, model_adv, pgd_linf, opt, loss_fn=cross_entropy, randomize=True)\n",
        "    model_ema.update_parameters(model_adv) # Update EMA model\n",
        "\n",
        "    test_err, test_loss = eval_epoch(dataloader_val_subset, model_ema, loss_fn=cross_entropy) # Evaluate clean acc. on EMA model\n",
        "    adv_err, adv_loss = eval_epoch_adversarial(dataloader_val_subset, model_ema, fgsm, loss_fn=cross_entropy) # Evaluate robust acc. on EMA model\n",
        "\n",
        "    # Update the losses and errors\n",
        "    log[\"train_losses\"] += [train_loss]\n",
        "    log[\"test_losses\"] += [test_loss]\n",
        "    log[\"adv_losses\"] += [adv_loss]\n",
        "    log[\"train_errors\"] += [train_err]\n",
        "    log[\"test_errors\"] += [test_err]\n",
        "    log[\"adv_errors\"] += [adv_err]\n",
        "\n",
        "    print(*(\"{:.6f}\".format(train_err),\n",
        "            \"{:.6f}\".format(test_err),\n",
        "            \"{:.6f}\".format(adv_err),\n",
        "            f\"{t+1}\",), sep=\"\\t\")\n",
        "\n",
        "###########################\n",
        "# STORE RESULTS\n",
        "store = False # set this variable to True when you have runs that you want to save\n",
        "if store:\n",
        "  with open(run_dir / \"log.json\", \"w\") as f:\n",
        "      json.dump(log, f)\n",
        "  torch.save(model_adv.state_dict(), run_dir / \"model_adv.pt\")"
      ],
      "metadata": {
        "id": "a52C46yiN7Y9",
        "outputId": "0e1a1281-087a-4e39-f205-ff2f987c7214",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin adversarial training run: weight_averaging\n",
            "\n",
            "TR      \tTE      \tADV     \tEpoch   \n",
            "0.752930\t0.890625\t0.897461\t1\n",
            "0.795410\t0.725586\t0.802734\t2\n",
            "0.688965\t0.882812\t0.892578\t3\n",
            "0.656250\t0.891602\t0.899414\t4\n",
            "0.634766\t0.901367\t0.901367\t5\n",
            "0.602051\t0.901367\t0.901367\t6\n",
            "0.576660\t0.901367\t0.901367\t7\n",
            "0.562012\t0.900391\t0.901367\t8\n",
            "0.538086\t0.881836\t0.889648\t9\n",
            "0.521973\t0.809570\t0.838867\t10\n",
            "0.502930\t0.740234\t0.783203\t11\n",
            "0.469727\t0.692383\t0.742188\t12\n",
            "0.452637\t0.687500\t0.728516\t13\n",
            "0.415527\t0.671875\t0.725586\t14\n",
            "0.384766\t0.652344\t0.715820\t15\n",
            "0.358887\t0.602539\t0.681641\t16\n",
            "0.334961\t0.575195\t0.660156\t17\n",
            "0.293457\t0.517578\t0.620117\t18\n",
            "0.267578\t0.516602\t0.617188\t19\n",
            "0.248047\t0.486328\t0.590820\t20\n",
            "0.213867\t0.474609\t0.587891\t21\n",
            "0.173828\t0.470703\t0.583008\t22\n",
            "0.167480\t0.479492\t0.583984\t23\n",
            "0.160645\t0.467773\t0.591797\t24\n",
            "0.117676\t0.458008\t0.574219\t25\n",
            "0.131348\t0.442383\t0.571289\t26\n",
            "0.102539\t0.440430\t0.570312\t27\n",
            "0.065430\t0.434570\t0.557617\t28\n",
            "0.037598\t0.413086\t0.551758\t29\n",
            "0.033203\t0.426758\t0.569336\t30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "x_rhDKq1E8qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Functions\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import contextlib\n",
        "from autoattack import AutoAttack\n",
        "\n",
        "# Sanity Check: Loading the log file and plotting adversarial accuracies over training epochs\n",
        "def sanity_check(model):\n",
        "  project_dir = Path('/content/MyDrive/adversarial_training')\n",
        "  with open(project_dir / model / \"log.json\", \"r\") as f:\n",
        "      log = json.load(f)\n",
        "\n",
        "  plt.plot(log[\"adv_errors\"])\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Robust Accuracy\")\n",
        "  return None\n",
        "\n",
        "# Evaluate Baseline Robust Accuracy and Final Robust Accuracy (after training) of a Model\n",
        "def evaluate_robust_accuracy(model, dataloader, attack):\n",
        "    model = resnet18()\n",
        "    model.conv1 = Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    model.fc = torch.nn.Linear(model_adv.fc.in_features, num_classes)\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    baseline_robust_accuracy = 1 - eval_epoch_adversarial(dataloader, model, attack, loss_fn=cross_entropy)[0]\n",
        "\n",
        "    # load weight file to evaluate trained model\n",
        "    model.load_state_dict(torch.load(weight_file, weights_only=True))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    final_robust_accuracy = 1 - eval_epoch_adversarial(dataloader, model, attack, loss_fn=cross_entropy)[0]\n",
        "    print(f\"Baseline Robust Accuracy: {baseline_robust_accuracy}\")\n",
        "    print(f\"Final Robust Accuracy: {final_robust_accuracy}\")\n",
        "    return baseline_robust_accuracy, final_robust_accuracy\n",
        "\n",
        "def evaluate(weight_file, dataloader, attack):\n",
        "    model = resnet18()\n",
        "    model.conv1 = Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    model.fc = torch.nn.Linear(model_adv.fc.in_features, num_classes)\n",
        "    model.eval()\n",
        "    model.load_state_dict(torch.load(weight_file, weights_only=True))\n",
        "    model = model.to(device)\n",
        "    clean_accuracy = 1 - eval_epoch(dataloader, model, loss_fn=cross_entropy)[0]\n",
        "    if attack == 'fgsm':\n",
        "        robust_accuracy = 1 - eval_epoch_adversarial(dataloader, model, fgsm, loss_fn=cross_entropy)[0]\n",
        "    elif attack == 'pgd':\n",
        "      robust_accuracy = 1 - eval_epoch_adversarial(dataloader, model, pgd_linf, loss_fn=cross_entropy)[0]\n",
        "    elif attack == 'autoattack':\n",
        "        x_test, y_test = next(iter(dataloader))\n",
        "        x_test, y_test = x_test.to(device), y_test.to(device)\n",
        "        output_buffer = io.StringIO()\n",
        "        with contextlib.redirect_stdout(output_buffer):\n",
        "            adversary = AutoAttack(model_adv, norm='Linf', eps=8/255, version='standard', device=device)\n",
        "            adversary.run_standard_evaluation(x_test, y_test, bs=128)\n",
        "\n",
        "        output = output_buffer.getvalue()\n",
        "\n",
        "        lines = output.splitlines()\n",
        "        for line in reversed(lines):\n",
        "            if \"robust accuracy:\" in line.lower():\n",
        "                robust_acc = float(line.split(\":\")[-1].strip().replace(\"%\", \"\")) / 100.0\n",
        "                break\n",
        "        else:\n",
        "            robust_acc = None\n",
        "    return clean_accuracy, robust_accuracy\n"
      ],
      "metadata": {
        "id": "sH_Ti-xquglD",
        "outputId": "48d84cfb-c748-45f4-f2c2-93a2848a42a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'autoattack'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-944960ebbaea>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautoattack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoAttack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Sanity Check: Loading the log file and plotting adversarial accuracies over training epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autoattack'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pareto plot of all models\n",
        "\n",
        "models_to_evaluate = ['adversarial_training','trades_loss','weight_averaging']\n",
        "clean_accuracies = []\n",
        "robust_accuracies = []\n",
        "# for plotting\n",
        "handles = []\n",
        "\n",
        "for model in models_to_evaluate:\n",
        "    weight_file = project_dir / model / \"model_adv.pt\"\n",
        "    attack = 'fgsm'\n",
        "    dataloader_test = dataloader_test_subset\n",
        "\n",
        "    clean_accuracy, robust_accuracy = evaluate(weight_file, dataloader_test, attack)\n",
        "    clean_accuracies.append(clean_accuracy)\n",
        "    robust_accuracies.append(robust_accuracy)\n",
        "    handle = plt.scatter(clean_accuracy, robust_accuracy, label=model)\n",
        "    handles.append(handle)\n",
        "\n",
        "plt.xlabel('Clean Accuracy')\n",
        "plt.ylabel('Robust Accuracy')\n",
        "plt.title('Pareto Plot of Models')\n",
        "plt.legend(handles=handles)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MUYIaGE8FI-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyCAOqCdy-Gs"
      },
      "outputs": [],
      "source": [
        "\"\"\" Regular Training \"\"\"\n",
        "import json\n",
        "from torch.nn import CrossEntropyLoss, Conv2d\n",
        "\n",
        "model_reg = resnet18()\n",
        "\n",
        "# CIFAR10: kernel_size 7 -> 3, stride 2 -> 1, padding 3->1\n",
        "model_reg.conv1 = Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_reg.fc = torch.nn.Linear(model_reg.fc.in_features, num_classes)\n",
        "\n",
        "pretrained_weights = torch.load(weight_file, weights_only=True)\n",
        "model_reg.load_state_dict(pretrained_weights)\n",
        "model_reg = model_reg.to(device)\n",
        "\n",
        "run_dir = project_dir / generate_run_name()\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "opt = SGD(model_reg.parameters(), lr=1e-1)\n",
        "opt = Adam(model_reg.parameters(), lr=1e-3)\n",
        "\n",
        "epochs = 2\n",
        "log = {key: [] for key in [\"train_losses\", \"test_losses\", \"adv_losses\",\n",
        "                           \"train_errors\", \"test_errors\", \"adv_errors\"]}\n",
        "\n",
        "print(f\"Begin adversarial training run: {run_dir.stem}\\n\")\n",
        "print(*(\"TR      \", \"TE      \", \"ADV     \", \"     \"), sep=\"\\t\")\n",
        "\n",
        "for t in range(epochs):\n",
        "    train_err, train_loss = train_epoch(dataloader_train, model_reg, opt)\n",
        "    test_err, test_loss = eval_epoch(dataloader_test, model_reg)\n",
        "    adv_err, adv_loss = eval_epoch_adversarial(dataloader_test, model_reg, fgsm)\n",
        "\n",
        "    # Update the losses and errors\n",
        "    log[\"train_losses\"] += [train_loss]\n",
        "    log[\"test_losses\"] += [test_loss]\n",
        "    log[\"adv_losses\"] += [adv_loss]\n",
        "    log[\"train_errors\"] += [train_err]\n",
        "    log[\"test_errors\"] += [test_err]\n",
        "    log[\"adv_errors\"] += [adv_err]\n",
        "\n",
        "    print(*(\"{:.6f}\".format(train_err),\n",
        "            \"{:.6f}\".format(test_err),\n",
        "            \"{:.6f}\".format(adv_err),\n",
        "            f\"Epoch: {t+1}\",), sep=\"\\t\")\n",
        "\n",
        "with open(run_dir / \"log.json\", \"w\") as f:\n",
        "    json.dump(log, f)\n",
        "torch.save(model_reg.state_dict(), run_dir / \"model_reg.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1lIob0L-uLue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ADV evaluated with FGSM\n",
        "We observe that the ADV validation error is lower than the training error because FGSM is a weaker attack thatn PGD\n",
        "Begin adversarial training run: magenta-slow-phoenix-20250105-1507\n",
        "\n",
        "TR      \tTE      \tADV     \tEpoch\n",
        "0.666760\t0.406778\t0.493000\t1\n",
        "0.539120\t0.314444\t0.405000\t2\n",
        "0.476300\t0.274778\t0.379000\t3\n",
        "0.432680\t0.245222\t0.339556\t4\n",
        "0.402620\t0.230444\t0.330000\t5\n",
        "\n",
        "ADV evaluated with PGD-Linf\n",
        "We observe that the ADV validation error is higher than the training error as expected\n",
        "Begin adversarial training run: orange-kind-shark-20250105-1531\n",
        "\n",
        "TR      \tTE      \tADV     \tEpoch\n",
        "0.675220\t0.419667\t0.599000\t1\n",
        "0.543500\t0.310889\t0.508778\t2\n",
        "0.472780\t0.271000\t0.475556\t3\n",
        "0.434380\t0.248000\t0.461889\t4\n",
        "0.403640\t0.230444\t0.457889\t5\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5tjaHMxCFwWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcUht5uBHlYG"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "gold-bright-eagle-20241204-1128:\n",
        "SGD(lr=1e-1), batch_size=1024\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "orange-kind-hawk-20241204-1148:\n",
        "SGD(lr=1e-1, weight_decay=5e-4), batch_size=1024\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "silver-shiny-phoenix-20241204-1210:\n",
        "SGD(lr=1e-1, weight_decay=5e-4, momentum=0.9, nesterov=True), batch_size=1024\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "gray-fierce-octopus-20241204-1226:\n",
        "SGD(lr=1e-1, weight_decay=5e-4, momentum=0.9, nesterov=True), batch_size=1024, CosineAnnealingLR\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "gold-loud-cheetah-20241204-1414:\n",
        "SGD(lr=1e-1, weight_decay=5e-4, momentum=0.9, nesterov=True), batch_size=1024,, epsilon=4/255\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "cyan-wise-eagle-20241203-1710:\n",
        "SGD(lr=1e-1, weight_decay=5e-4, momentum=0.9, nesterov=True), batch_size=1024,, epsilon=32/255\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-ys2JFSidFI5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
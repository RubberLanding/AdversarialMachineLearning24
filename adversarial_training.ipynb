{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RubberLanding/AdversarialMachineLearning24/blob/nico/adversarial_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preliminaries\n"
      ],
      "metadata": {
        "id": "JN4iX5SEZ1Aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install all needed packages."
      ],
      "metadata": {
        "id": "-bTdL_l2Z6LQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWRfFtr2j9T-",
        "outputId": "6a996431-885a-4bb2-ff41-f29cae0696c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
            "Collecting git+https://github.com/fra31/auto-attack\n",
            "  Cloning https://github.com/fra31/auto-attack to /tmp/pip-req-build-s7ey_dpp\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/fra31/auto-attack /tmp/pip-req-build-s7ey_dpp\n",
            "  Resolved https://github.com/fra31/auto-attack to commit a39220048b3c9f2cca9a4d3a54604793c68eca7e\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install git+https://github.com/fra31/auto-attack\n",
        "!pip install statsmodels\n",
        "# !git pull https://github.com/RubberLanding/AdversarialMachineLearning24"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the weights for the pre-trained Resnet18 on CIFAR-10. We only do this once and store them on a private GoogleDrive, which we later import to be able to actually load the weights."
      ],
      "metadata": {
        "id": "3BunrBffT8NG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgFph_IRo8BW",
        "outputId": "d5b6e08f-3c98-4337-b078-204e0f122b1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=17fmN8eQdLpq2jIMQ_X0IXDPXfI9oVWgq\n",
            "From (redirected): https://drive.google.com/uc?id=17fmN8eQdLpq2jIMQ_X0IXDPXfI9oVWgq&confirm=t&uuid=ccb31ef0-0748-4558-8175-1e76db1fcb31\n",
            "To: /content/state_dicts.zip\n",
            "100% 979M/979M [00:07<00:00, 125MB/s]\n"
          ]
        }
      ],
      "source": [
        "# \"\"\"Download pre-trained weights for ResNet18 on CIFAR10\"\"\"\n",
        "# !pip install gdown\n",
        "\n",
        "# # Source: https://github.com/huyvnphan/PyTorch_CIFAR10\n",
        "# FILE_ID = \"17fmN8eQdLpq2jIMQ_X0IXDPXfI9oVWgq\"\n",
        "# file_url = f\"https://drive.google.com/uc?id={FILE_ID}\"\n",
        "\n",
        "# !gdown {file_url}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all packages und functions we need."
      ],
      "metadata": {
        "id": "z95qM1xeZ-g2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DIOCbEGHlAu9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.optim import SGD, Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount your GoogleDrive where you stored the weights for the pre-trained Resnet18 before. We will also store the logging files and model weights there."
      ],
      "metadata": {
        "id": "8KJEoOzHUIjv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX34q7fo-l5R",
        "outputId": "faa3820b-5e83-44fa-b4c6-7422ffca96b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "project_dir = Path('/content/drive/MyDrive/adversarial_training')\n",
        "project_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "weight_dir = project_dir / \"weights\"\n",
        "weight_dir.mkdir(parents=True, exist_ok=True)\n",
        "weight_file = weight_dir / \"resnet18.pt\"\n",
        "\n",
        "# \"\"\"Extract the pre-trained model weights to Google Drive\"\"\"\n",
        "# with zipfile.ZipFile(\"state_dicts.zip\", \"r\") as zip_ref:\n",
        "#   # print(zip_ref.namelist())\n",
        "#   with zip_ref.open(\"state_dicts/resnet18.pt\") as zf, open(weight_file, 'wb') as f:\n",
        "#       shutil.copyfileobj(zf, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a convenience function to name the different training runs."
      ],
      "metadata": {
        "id": "DYM2QScUaE2z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bqVEbYQFJS_c"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "def generate_run_name():\n",
        "    \"\"\"Generate a random name for a run.\"\"\"\n",
        "    colors = [\n",
        "        \"red\", \"blue\", \"green\", \"yellow\", \"purple\", \"orange\", \"pink\",\n",
        "        \"black\", \"white\", \"gray\", \"silver\", \"gold\", \"cyan\", \"magenta\"]\n",
        "    adjectives = [\n",
        "        \"fast\", \"slow\", \"shiny\", \"dull\", \"bright\", \"dark\", \"silent\",\n",
        "        \"loud\", \"brave\", \"calm\", \"wise\", \"fierce\", \"kind\", \"strong\"]\n",
        "    nouns = [\n",
        "        \"dragon\", \"tiger\", \"lion\", \"panda\", \"wolf\", \"phoenix\", \"eagle\",\n",
        "        \"fox\", \"bear\", \"shark\", \"hawk\", \"cheetah\", \"whale\", \"octopus\"]\n",
        "    color = random.choice(colors)\n",
        "    adjective = random.choice(adjectives)\n",
        "    noun = random.choice(nouns)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "    run_name = f\"{color}-{adjective}-{noun}-{timestamp}\"\n",
        "    return run_name"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "Download the CIFAR-10 data. Split it into training, validation and test set. Do some pre-processing."
      ],
      "metadata": {
        "id": "HgXUQL_saQG7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLo5cWwgkquv",
        "outputId": "eb37a207-e4bb-4e4f-9180-28b5981a1cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to datasets/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 33.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/cifar-10-python.tar.gz to datasets\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "batch_size = 1024 # batch size has to be < 2**16, should be <= 2**13 for T4\n",
        "debug = True\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.491, 0.482, 0.446], std=[0.247, 0.243, 0.261]),\n",
        "    ])\n",
        "\n",
        "\"\"\" Load data \"\"\"\n",
        "data_train = CIFAR10(root=\"datasets\", train=True, download=True, transform=transform)\n",
        "data_test = CIFAR10(root=\"datasets\", train=False, download=True, transform=transform)\n",
        "data_test, data_val = torch.utils.data.random_split(data_test, [0.1, 0.9])\n",
        "\n",
        "dataloader_train = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
        "dataloader_test = DataLoader(data_test, batch_size=len(data_test), shuffle=False) # create test dataloader with a single batch\n",
        "dataloader_val = DataLoader(data_val, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "num_classes = len(data_train.classes)\n",
        "\n",
        "# mean = data_train.data.mean(axis=(0,1,2)) / 255 # [0.49139968, 0.48215841, 0.44653091]\n",
        "# std = data_train.data.std(axis=(0,1,2)) / 255 # [0.24703223, 0.24348513, 0.26158784]\n",
        "\n",
        "data_train_subset = Subset(data_train, list(range(2*batch_size)))\n",
        "data_val_subset = Subset(data_val, list(range(2)))\n",
        "data_test_subset = Subset(data_test, list(range(100)))\n",
        "\n",
        "dataloader_train_subset = DataLoader(data_train_subset, batch_size=batch_size, shuffle=True)\n",
        "dataloader_val_subset = DataLoader(data_val_subset, batch_size=len(data_val_subset), shuffle=False)\n",
        "dataloader_test_subset = DataLoader(data_test_subset, batch_size=len(data_test_subset), shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attacks\n",
        "Define the attacks."
      ],
      "metadata": {
        "id": "TLTLCMphadmb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GSHC7WdoyxSC"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def fgsm(model, X, y, epsilon=8/255):\n",
        "    \"\"\" Construct FGSM adversarial examples on the examples X\"\"\"\n",
        "    delta = torch.zeros_like(X, requires_grad=True)\n",
        "    loss = CrossEntropyLoss()(model(X + delta), y)\n",
        "    loss.backward()\n",
        "    return epsilon * delta.grad.detach().sign()\n",
        "\n",
        "def pgd_linf(model, X, y, epsilon=8/255, alpha=2/255, num_iter=10, randomize=False):\n",
        "    \"\"\" Construct FGSM adversarial examples on the examples X\"\"\"\n",
        "    if randomize:\n",
        "        delta = torch.rand_like(X, requires_grad=True)\n",
        "        delta.data = delta.data * 2 * epsilon - epsilon\n",
        "    else:\n",
        "        delta = torch.zeros_like(X, requires_grad=True)\n",
        "\n",
        "    for t in range(num_iter):\n",
        "        loss = CrossEntropyLoss()(model(X + delta), y)\n",
        "        loss.backward()\n",
        "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
        "        delta.grad.zero_()\n",
        "    return delta.detach()\n",
        "\n",
        "def pgd_linf_trades(model, X, y, epsilon=8/255, alpha=2/255, num_iter=10, randomize=False):\n",
        "    \"\"\" Construct FGSM adversarial examples with KL-Divergence for TRADES.\n",
        "        Should be used together with traded_loss().\n",
        "    \"\"\"\n",
        "    if randomize:\n",
        "        delta = torch.rand_like(X, requires_grad=True)\n",
        "        delta.data = delta.data * 2 * epsilon - epsilon\n",
        "    else:\n",
        "        delta = torch.zeros_like(X, requires_grad=True)\n",
        "\n",
        "    for t in range(num_iter):\n",
        "        # maybe set log_target=True and pass F.log_softmax(model(X), dim=1),\n",
        "        # see the docs for more details\n",
        "        loss = F.kl_div(F.log_softmax(model(X + delta), dim=1),\n",
        "                        F.softmax(model(X), dim=1),\n",
        "                        reduction='batchmean')\n",
        "        loss.backward()\n",
        "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
        "        delta.grad.zero_()\n",
        "    return delta.detach()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Losses\n",
        "We define a wrapper, that calls the appropriate loss function with the correct arguments. Inside the wrapper, we implement the loss functions."
      ],
      "metadata": {
        "id": "3cPBOyOCkiLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LossWrapper:\n",
        "    def __init__(self, loss_fn, lambda_tradeoff=1.0):\n",
        "        self.loss_fn = loss_fn\n",
        "        self.lambda_tradeoff = lambda_tradeoff\n",
        "\n",
        "    def __call__(self, model, X, y, delta=0.0):\n",
        "        \"\"\"Args:\n",
        "              model: The model being trained.\n",
        "              X: Clean input data.\n",
        "              delta: Perturbations applied to X.\n",
        "              y: Ground-truth labels.\n",
        "        \"\"\"\n",
        "        # Cross-Entropy Loss\n",
        "        if self.loss_fn == \"CE\":\n",
        "            yp = model(X + delta)\n",
        "            return F.cross_entropy(yp, y)\n",
        "\n",
        "        # TRADES Loss\n",
        "        elif self.loss_fn == \"TRADES\":\n",
        "            yp_adv = model(X + delta)\n",
        "            yp_clean = model(X)\n",
        "            clean_loss = F.cross_entropy(yp_clean, y)\n",
        "            robust_loss = F.kl_div(F.log_softmax(yp_adv, dim=1),\n",
        "                            F.softmax(yp_clean, dim=1),\n",
        "                            reduction='batchmean')\n",
        "            return clean_loss + self.lambda_tradeoff * robust_loss\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported loss function\")"
      ],
      "metadata": {
        "id": "9XMbYBVLkkWr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "rSjV2ajnb_Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define functions that train a model."
      ],
      "metadata": {
        "id": "K5-e2T8AcHZV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Dxm_8JdYoK3D"
      },
      "outputs": [],
      "source": [
        "def train_epoch(loader, model, opt, loss_fn):\n",
        "    \"\"\"Standard training/evaluation epoch over the dataset\"\"\"\n",
        "    model.train()\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        yp = model(X)\n",
        "        loss = loss_fn(model, X, y)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(loader, model, loss_fn):\n",
        "    \"\"\"Standard training/evaluation epoch over the dataset\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        yp = model(X)\n",
        "        loss = loss_fn(model, X, y)\n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n",
        "def train_epoch_adversarial(loader, model, attack, opt, loss_fn, **kwargs):\n",
        "    \"\"\"Adversarial training/evaluation epoch over the dataset\"\"\"\n",
        "    model.train()\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        delta = attack(model, X, y, **kwargs)\n",
        "        yp = model(X+delta)\n",
        "        loss = loss_fn(model, X, y, delta)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n",
        "def eval_epoch_adversarial(loader, model, attack, loss_fn, **kwargs):\n",
        "    \"\"\"Adversarial training/evaluation epoch over the dataset\"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss, total_err = 0.0, 0.0\n",
        "\n",
        "    for X, y in loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute adversarial perturbations (requires gradients)\n",
        "        with torch.enable_grad():\n",
        "            delta = attack(model, X, y, **kwargs)\n",
        "\n",
        "        # Evaluate the model on adversarial examples without gradients\n",
        "        with torch.no_grad():\n",
        "            yp = model(X + delta)\n",
        "            loss = loss_fn(model, X, y, delta)\n",
        "\n",
        "            total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "            total_loss += loss.item() * X.shape[0]\n",
        "\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n",
        "def train_epoch_rs(loader, model, opt, loss_fn, sigma=0.25):\n",
        "    \"\"\"Training epoch with Gaussian noise over the dataset for Randomized Smoothing\"\"\"\n",
        "    model.train()\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        yp = model(X + torch.randn_like(X) * sigma) # Add Gaussian noise\n",
        "        loss = loss_fn(model, X, y)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "zqdzqrZxNRYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the Advarsarial Training Model\n"
      ],
      "metadata": {
        "id": "oRmrIIaWd4rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from torch.nn import CrossEntropyLoss, Conv2d\n",
        "\n",
        "###########################\n",
        "# LOAD THE MODEL\n",
        "\n",
        "model_adv = resnet18()\n",
        "\n",
        "# CIFAR10: kernel_size 7 -> 3, stride 2 -> 1, padding 3->1\n",
        "model_adv.conv1 = Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_adv.fc = torch.nn.Linear(model_adv.fc.in_features, num_classes)\n",
        "\n",
        "pretrained_weights = torch.load(weight_file, weights_only=True)\n",
        "model_adv.load_state_dict(pretrained_weights)\n",
        "model_adv = model_adv.to(device)\n",
        "\n",
        "###########################\n",
        "# SET LOGGING\n",
        "\n",
        "run_dir = project_dir / generate_run_name()\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "log = {key: [] for key in [\"train_losses\", \"test_losses\", \"adv_losses\",\n",
        "                           \"train_errors\", \"test_errors\", \"adv_errors\"]}\n"
      ],
      "metadata": {
        "id": "hLdE6jITd8PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Adversarial Training Model\n",
        "Use adversarial training to train the robust model."
      ],
      "metadata": {
        "id": "xh7mVivIZqA6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNAZgohWXLfM",
        "outputId": "95420750-4dbf-4305-acc7-2311587b9091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin adversarial training run: orange-fierce-shark-20250110-1652\n",
            "\n",
            "TR      \tTE      \tADV     \tEpoch   \n",
            "0.626953\t0.881836\t0.900391\t1\n",
            "0.663086\t0.654297\t0.766602\t2\n",
            "0.527832\t0.529297\t0.693359\t3\n",
            "0.488281\t0.471680\t0.630859\t4\n",
            "0.428223\t0.421875\t0.595703\t5\n",
            "0.390625\t0.392578\t0.596680\t6\n",
            "0.351074\t0.377930\t0.587891\t7\n",
            "0.308105\t0.366211\t0.580078\t8\n",
            "0.262695\t0.351562\t0.568359\t9\n",
            "0.233887\t0.329102\t0.559570\t10\n",
            "0.197266\t0.332031\t0.552734\t11\n",
            "0.170898\t0.321289\t0.539062\t12\n",
            "0.141113\t0.310547\t0.532227\t13\n",
            "0.111328\t0.318359\t0.533203\t14\n",
            "0.080566\t0.323242\t0.543945\t15\n"
          ]
        }
      ],
      "source": [
        "###########################\n",
        "# SET TRAINING PARAMETERS\n",
        "\n",
        "opt = Adam(model_adv.parameters(), lr=1e-3)\n",
        "# opt = SGD(model_adv.parameters(), lr=1e-1)\n",
        "# scheduler = CosineAnnealingLR(opt, T_max=100)\n",
        "# scheduler = CosineAnnealingWarmRestarts(opt, T_0=10, T_mult=2, eta_min=0)\n",
        "\n",
        "epochs = 15\n",
        "\n",
        "trades = LossWrapper(\"TRADES\")\n",
        "cross_entropy = LossWrapper(\"CE\")\n",
        "###########################\n",
        "# START TRAINING\n",
        "\n",
        "print(f\"Begin adversarial training run: {run_dir.stem}\\n\")\n",
        "print(*(\"TR      \", \"TE      \", \"ADV     \", \"Epoch   \"), sep=\"\\t\")\n",
        "\n",
        "for t in range(epochs):\n",
        "    train_err, train_loss = train_epoch_adversarial(dataloader_train_subset, model_adv, pgd_linf_trades, opt, loss_fn=trades)\n",
        "    test_err, test_loss = eval_epoch(dataloader_val_subset, model_adv, loss_fn=cross_entropy)\n",
        "    adv_err, adv_loss = eval_epoch_adversarial(dataloader_val_subset, model_adv, fgsm, loss_fn=cross_entropy)\n",
        "\n",
        "    # Update the losses and errors\n",
        "    log[\"train_losses\"] += [train_loss]\n",
        "    log[\"test_losses\"] += [test_loss]\n",
        "    log[\"adv_losses\"] += [adv_loss]\n",
        "    log[\"train_errors\"] += [train_err]\n",
        "    log[\"test_errors\"] += [test_err]\n",
        "    log[\"adv_errors\"] += [adv_err]\n",
        "\n",
        "    print(*(\"{:.6f}\".format(train_err),\n",
        "            \"{:.6f}\".format(test_err),\n",
        "            \"{:.6f}\".format(adv_err),\n",
        "            f\"{t+1}\",), sep=\"\\t\")\n",
        "\n",
        "###########################\n",
        "# STORE RESULTS\n",
        "store = False # set this variable to True when you have runs that you want to save\n",
        "if store:\n",
        "  with open(run_dir / \"log.json\", \"w\") as f:\n",
        "      json.dump(log, f)\n",
        "  torch.save(model_adv.state_dict(), run_dir / \"model_adv.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install AutoAttack to calculate a robust accuracy for the model to get a fair comparison, e.g. with models from [RobustBench](https://robustbench.github.io/)."
      ],
      "metadata": {
        "id": "n0Y3Q_ZlZy3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/fra31/auto-attack"
      ],
      "metadata": {
        "id": "3CJs5EBFu26D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autoattack import AutoAttack\n",
        "\n",
        "X, y = next(iter(dataloader_test)) # optimally, the test loader has a single batch containing the entire test set\n",
        "X, y = X.to(device), y.to(device)\n",
        "\n",
        "adversary = AutoAttack(model_adv, norm='Linf', eps=8/255, version='standard')\n",
        "adv_examples = adversary.run_standard_evaluation(X, y, bs=batch_size) # this takes ~8min on T4 on a single test batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uknpMFbi73zO",
        "outputId": "c1c7d7a6-5fc1-4fb7-a49c-c8dd79c84c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "setting parameters for standard version\n",
            "using standard version including apgd-ce, apgd-t, fab-t, square.\n",
            "initial accuracy: 58.30%\n",
            "apgd-ce - 1/5 - 35 out of 128 successfully perturbed\n",
            "apgd-ce - 2/5 - 29 out of 128 successfully perturbed\n",
            "apgd-ce - 3/5 - 32 out of 128 successfully perturbed\n",
            "apgd-ce - 4/5 - 31 out of 128 successfully perturbed\n",
            "apgd-ce - 5/5 - 23 out of 71 successfully perturbed\n",
            "robust accuracy after APGD-CE: 43.30% (total time 16.5 s)\n",
            "apgd-t - 1/4 - 17 out of 128 successfully perturbed\n",
            "apgd-t - 2/4 - 14 out of 128 successfully perturbed\n",
            "apgd-t - 3/4 - 16 out of 128 successfully perturbed\n",
            "apgd-t - 4/4 - 7 out of 49 successfully perturbed\n",
            "robust accuracy after APGD-T: 37.90% (total time 122.8 s)\n",
            "fab-t - 1/3 - 3 out of 128 successfully perturbed\n",
            "fab-t - 2/3 - 2 out of 128 successfully perturbed\n",
            "fab-t - 3/3 - 2 out of 123 successfully perturbed\n",
            "robust accuracy after FAB-T: 37.20% (total time 285.0 s)\n",
            "square - 1/3 - 1 out of 128 successfully perturbed\n",
            "square - 2/3 - 1 out of 128 successfully perturbed\n",
            "square - 3/3 - 2 out of 116 successfully perturbed\n",
            "robust accuracy after SQUARE: 36.80% (total time 446.3 s)\n",
            "Warning: Square Attack has decreased the robust accuracy of 0.40%. This might indicate that the robustness evaluation using AutoAttack is unreliable. Consider running Square Attack with more iterations and restarts or an adaptive attack. See flags_doc.md for details.\n",
            "max Linf perturbation: 1.98785, nan in tensor: 0, max: 2.13169, min: -1.98785\n",
            "robust accuracy: 36.80%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the Weight Averaging Model\n",
        "Here we initialize an exponential moving average (EMA) model, based on the robust model."
      ],
      "metadata": {
        "id": "O32SQzimNaFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from torch.nn import CrossEntropyLoss, Conv2d\n",
        "\n",
        "###########################\n",
        "# LOAD THE MODEL\n",
        "\n",
        "model_adv = resnet18()\n",
        "\n",
        "# CIFAR10: kernel_size 7 -> 3, stride 2 -> 1, padding 3->1\n",
        "model_adv.conv1 = Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_adv.fc = torch.nn.Linear(model_adv.fc.in_features, num_classes)\n",
        "\n",
        "pretrained_weights = torch.load(weight_file, weights_only=True)\n",
        "model_adv.load_state_dict(pretrained_weights)\n",
        "\n",
        "model_ema = torch.optim.swa_utils.AveragedModel(model_adv, multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.9))\n",
        "\n",
        "model_adv = model_adv.to(device)\n",
        "model_ema = model_ema.to(device)\n",
        "\n",
        "###########################\n",
        "# SET LOGGING\n",
        "\n",
        "run_dir = project_dir / generate_run_name()\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "log = {key: [] for key in [\"train_losses\", \"test_losses\", \"adv_losses\",\n",
        "                           \"train_errors\", \"test_errors\", \"adv_errors\"]}\n"
      ],
      "metadata": {
        "id": "3Ed0KCUxMdQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FML825DiaHuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Weight Averaged Model"
      ],
      "metadata": {
        "id": "FEf0SlFQaN0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# SET TRAINING PARAMETERS\n",
        "\n",
        "opt = Adam(model_adv.parameters(), lr=1e-3)\n",
        "# opt = SGD(model_adv.parameters(), lr=1e-1)\n",
        "# scheduler = CosineAnnealingLR(opt, T_max=100)\n",
        "# scheduler = CosineAnnealingWarmRestarts(opt, T_0=10, T_mult=2, eta_min=0)\n",
        "\n",
        "epochs = 15\n",
        "\n",
        "trades = LossWrapper(\"TRADES\")\n",
        "cross_entropy = LossWrapper(\"CE\")\n",
        "\n",
        "###########################\n",
        "# START TRAINING\n",
        "\n",
        "print(f\"Begin adversarial training run: {run_dir.stem}\\n\")\n",
        "print(*(\"TR      \", \"TE      \", \"ADV     \", \"Epoch   \"), sep=\"\\t\")\n",
        "\n",
        "for t in range(epochs):\n",
        "    train_err, train_loss = train_epoch_adversarial(dataloader_train_subset, model_adv, pgd_linf, opt, loss_fn=cross_entropy)\n",
        "    model_ema.update_parameters(model_adv) # Update EMA model\n",
        "\n",
        "    test_err, test_loss = eval_epoch(dataloader_val_subset, model_ema, loss_fn=cross_entropy) # Evaluate clean acc. on EMA model\n",
        "    adv_err, adv_loss = eval_epoch_adversarial(dataloader_val_subset, model_ema, fgsm, loss_fn=cross_entropy) # Evaluate robust acc. on EMA model\n",
        "\n",
        "    # Update the losses and errors\n",
        "    log[\"train_losses\"] += [train_loss]\n",
        "    log[\"test_losses\"] += [test_loss]\n",
        "    log[\"adv_losses\"] += [adv_loss]\n",
        "    log[\"train_errors\"] += [train_err]\n",
        "    log[\"test_errors\"] += [test_err]\n",
        "    log[\"adv_errors\"] += [adv_err]\n",
        "\n",
        "    print(*(\"{:.6f}\".format(train_err),\n",
        "            \"{:.6f}\".format(test_err),\n",
        "            \"{:.6f}\".format(adv_err),\n",
        "            f\"{t+1}\",), sep=\"\\t\")\n",
        "\n",
        "###########################\n",
        "# STORE RESULTS\n",
        "store = False # set this variable to True when you have runs that you want to save\n",
        "if store:\n",
        "  with open(run_dir / \"log.json\", \"w\") as f:\n",
        "      json.dump(log, f)\n",
        "  torch.save(model_adv.state_dict(), run_dir / \"model_adv.pt\")\n",
        "  torch.save(model_ema.state_dict(), run_dir / \"model_ema.pt\")"
      ],
      "metadata": {
        "id": "a52C46yiN7Y9",
        "outputId": "da6f6df0-1aa5-42af-9af1-588ac9af7218",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin adversarial training run: gray-loud-fox-20250110-1808\n",
            "\n",
            "TR      \tTE      \tADV     \tEpoch   \n",
            "0.732910\t0.805664\t0.852539\t1\n",
            "0.758301\t0.713867\t0.785156\t2\n",
            "0.650391\t0.889648\t0.894531\t3\n",
            "0.620605\t0.897461\t0.898438\t4\n",
            "0.579102\t0.893555\t0.895508\t5\n",
            "0.544922\t0.886719\t0.890625\t6\n",
            "0.514648\t0.884766\t0.887695\t7\n",
            "0.481934\t0.871094\t0.879883\t8\n",
            "0.457031\t0.828125\t0.862305\t9\n",
            "0.412598\t0.760742\t0.809570\t10\n",
            "0.385742\t0.684570\t0.737305\t11\n",
            "0.358398\t0.646484\t0.712891\t12\n",
            "0.323730\t0.648438\t0.708008\t13\n",
            "0.275879\t0.602539\t0.669922\t14\n",
            "0.239746\t0.542969\t0.632812\t15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Randomized Smoothing"
      ],
      "metadata": {
        "id": "DfLDKI1QwgeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.proportion import proportion_confint\n",
        "from scipy.stats import norm, binomtest\n",
        "\n",
        "class RandomizedSmoother(object):\n",
        "\n",
        "  def __init__(self, base_model, num_classes, sigma=0.25, epsilon=8/255, alpha=0.05, norm=\"L2\"):\n",
        "    self.base_model = base_model\n",
        "    self.num_classes = num_classes\n",
        "    self.sigma = sigma\n",
        "    self.epsilon = epsilon\n",
        "    self.norm = norm\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def predict(self, X, num_samples=100):\n",
        "    batch_size = X.size(0)\n",
        "    # Compute class counts for noisy samples\n",
        "    class_counts = self.get_noisy_counts(X, num_samples)\n",
        "\n",
        "    # Sort class counts to find top 2 most frequent classes\n",
        "    sorted_counts_vals, sorted_counts_ind = class_counts.sort()\n",
        "    top1_class = sorted_counts_ind[:, -1]  # Most frequent class\n",
        "    top2_class = sorted_counts_ind[:, -2]  # Second most frequent class\n",
        "    top1_class_count = sorted_counts_vals[:, -1]\n",
        "    top2_class_count = sorted_counts_vals[:, -2]\n",
        "\n",
        "    predictions = torch.zeros(batch_size, device=X.device)\n",
        "    for i in range(batch_size):\n",
        "        # Use binomial test to check if the top class is significantly more frequent\n",
        "        n_a, n_b = top1_class_count[i], top2_class_count[i]\n",
        "        predictions[i] = top1_class[i] if binomtest(n_a, n_a + n_b, p=0.5).pvalue <= self.alpha else float('nan')\n",
        "\n",
        "    return predictions\n",
        "\n",
        "  def certify(self, X, num_samples=100):\n",
        "    batch_size = X.size(0)\n",
        "    # Perform two sampling procedures to avoid selection bias\n",
        "    counts_a = self.get_noisy_counts(X, num_samples)\n",
        "    top1_class = counts_a.argmax(dim=-1).unsqueeze(1)  # Get most frequent class index\n",
        "\n",
        "    counts = self.get_noisy_counts(X, num_samples)\n",
        "    # Extract the counts for the most frequent class\n",
        "    n_a = counts.gather(dim=1, index=top1_class).squeeze()\n",
        "\n",
        "    certified_radii = torch.zeros(batch_size)\n",
        "    for i in range(batch_size):\n",
        "        # Compute lower confidence bound for the top class probability\n",
        "        conf_bound, _ = proportion_confint(n_a[i], num_samples, alpha=2 * self.alpha, method=\"beta\")\n",
        "        # Calculate the certified radius based on the confidence bound\n",
        "        certified_radii[i] = self.get_radius(conf_bound, self.norm) if conf_bound >= 0.5 else float(\"nan\")\n",
        "\n",
        "    return certified_radii\n",
        "\n",
        "  def get_noisy_counts(self, X, num_samples):\n",
        "    batch_size = X.size(0)\n",
        "    class_counts = torch.zeros(batch_size, self.num_classes, dtype=torch.int64, device=X.device)\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # Generate noisy samples based on the specified norm\n",
        "        noisy_samples = X + self.get_noise(X.shape, self.norm)\n",
        "        with torch.no_grad():\n",
        "            # Predict classes for noisy samples\n",
        "            logits = self.base_model(noisy_samples)\n",
        "            class_pred = F.softmax(logits, dim=-1).argmax(dim=-1)\n",
        "            # Update class counts\n",
        "            for i in range(batch_size):\n",
        "                class_counts[i, class_pred[i]] += 1\n",
        "\n",
        "    return class_counts\n",
        "\n",
        "  def get_noise(self, shape, p_norm=\"L2\"):\n",
        "    if p_norm==\"L2\":\n",
        "      return torch.randn(shape) * self.sigma\n",
        "    elif p_norm==\"Linf\":\n",
        "      return 2 * self.epsilon * torch.rand(shape) - self.epsilon\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported norm.\")\n",
        "\n",
        "  def get_radius(self, conf_bound, p_norm=\"L2\"):\n",
        "    if p_norm==\"L2\":\n",
        "      return self.sigma * norm.ppf(conf_bound)\n",
        "    elif p_norm==\"Linf\":\n",
        "      return self.epsilon * (2 * conf_bound - 1)\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported norm.\")\n"
      ],
      "metadata": {
        "id": "9LUFmmFsxTyA"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from torch.nn import CrossEntropyLoss, Conv2d\n",
        "\n",
        "###########################\n",
        "# LOAD THE MODEL\n",
        "\n",
        "model_base = resnet18()\n",
        "\n",
        "# CIFAR10: kernel_size 7 -> 3, stride 2 -> 1, padding 3->1\n",
        "model_base.conv1 = Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_base.fc = torch.nn.Linear(model_base.fc.in_features, num_classes)\n",
        "\n",
        "pretrained_weights = torch.load(weight_file, weights_only=True)\n",
        "model_base.load_state_dict(pretrained_weights)\n",
        "model_base = model_base.to(device)\n",
        "\n",
        "###########################\n",
        "# SET LOGGING\n",
        "\n",
        "run_dir = project_dir / generate_run_name()\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "log = {key: [] for key in [\"train_losses\", \"test_losses\",\n",
        "                           \"train_errors\", \"test_errors\"]}\n"
      ],
      "metadata": {
        "id": "8qVgeFAQwf0V"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# SET TRAINING PARAMETERS\n",
        "\n",
        "opt = Adam(model_adv.parameters(), lr=1e-3)\n",
        "# opt = SGD(model_adv.parameters(), lr=1e-1)\n",
        "\n",
        "epochs = 15\n",
        "cross_entropy = LossWrapper(\"CE\")\n",
        "\n",
        "###########################\n",
        "# START TRAINING\n",
        "\n",
        "print(f\"Begin adversarial training run: {run_dir.stem}\\n\")\n",
        "print(*(\"TR      \", \"TE      \", \"ADV     \", \"Epoch   \"), sep=\"\\t\")\n",
        "\n",
        "for t in range(epochs):\n",
        "    train_err, train_loss = train_epoch(dataloader_train_subset, model_base, opt, loss_fn=cross_entropy)\n",
        "    test_err, test_loss = eval_epoch(dataloader_val_subset, model_base, loss_fn=cross_entropy)\n",
        "\n",
        "    # Update the losses and errors\n",
        "    log[\"train_losses\"] += [train_loss]\n",
        "    log[\"test_losses\"] += [test_loss]\n",
        "    log[\"train_errors\"] += [train_err]\n",
        "    log[\"test_errors\"] += [test_err]\n",
        "\n",
        "    print(*(\"{:.6f}\".format(train_err),\n",
        "            \"{:.6f}\".format(test_err),\n",
        "            f\"{t+1}\",), sep=\"\\t\")\n",
        "\n",
        "###########################\n",
        "# STORE RESULTS\n",
        "store = False # set this variable to True when you have runs that you want to save\n",
        "if store:\n",
        "  with open(run_dir / \"log.json\", \"w\") as f:\n",
        "      json.dump(log, f)\n",
        "  torch.save(model_base.state_dict(), run_dir / \"model_base.pt\")\n"
      ],
      "metadata": {
        "id": "rz_DpN9dnhaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 100\n",
        "sigma = 0.25\n",
        "alpha = 0.05\n",
        "\n",
        "X, y = next(iter(dataloader_val_subset))\n",
        "X, y = X.to(device), y.to(device)\n",
        "\n",
        "model_rs = RandomizedSmoother(model_base, num_classes)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fF_g1Nro9vv",
        "outputId": "b4d2dda6-2c04-469a-fb9f-943874fe6ed5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3., 9.])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regular Training"
      ],
      "metadata": {
        "id": "26k9eu-vmZtk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyCAOqCdy-Gs"
      },
      "outputs": [],
      "source": [
        "\"\"\" Regular Training \"\"\"\n",
        "import json\n",
        "from torch.nn import CrossEntropyLoss, Conv2d\n",
        "\n",
        "model_reg = resnet18()\n",
        "\n",
        "# CIFAR10: kernel_size 7 -> 3, stride 2 -> 1, padding 3->1\n",
        "model_reg.conv1 = Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_reg.fc = torch.nn.Linear(model_reg.fc.in_features, num_classes)\n",
        "\n",
        "pretrained_weights = torch.load(weight_file, weights_only=True)\n",
        "model_reg.load_state_dict(pretrained_weights)\n",
        "model_reg = model_reg.to(device)\n",
        "\n",
        "run_dir = project_dir / generate_run_name()\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "opt = SGD(model_reg.parameters(), lr=1e-1)\n",
        "opt = Adam(model_reg.parameters(), lr=1e-3)\n",
        "\n",
        "epochs = 2\n",
        "log = {key: [] for key in [\"train_losses\", \"test_losses\", \"adv_losses\",\n",
        "                           \"train_errors\", \"test_errors\", \"adv_errors\"]}\n",
        "\n",
        "print(f\"Begin adversarial training run: {run_dir.stem}\\n\")\n",
        "print(*(\"TR      \", \"TE      \", \"ADV     \", \"     \"), sep=\"\\t\")\n",
        "\n",
        "for t in range(epochs):\n",
        "    train_err, train_loss = train_epoch(dataloader_train, model_reg, opt)\n",
        "    test_err, test_loss = eval_epoch(dataloader_test, model_reg)\n",
        "    adv_err, adv_loss = eval_epoch_adversarial(dataloader_test, model_reg, fgsm)\n",
        "\n",
        "    # Update the losses and errors\n",
        "    log[\"train_losses\"] += [train_loss]\n",
        "    log[\"test_losses\"] += [test_loss]\n",
        "    log[\"adv_losses\"] += [adv_loss]\n",
        "    log[\"train_errors\"] += [train_err]\n",
        "    log[\"test_errors\"] += [test_err]\n",
        "    log[\"adv_errors\"] += [adv_err]\n",
        "\n",
        "    print(*(\"{:.6f}\".format(train_err),\n",
        "            \"{:.6f}\".format(test_err),\n",
        "            \"{:.6f}\".format(adv_err),\n",
        "            f\"Epoch: {t+1}\",), sep=\"\\t\")\n",
        "\n",
        "with open(run_dir / \"log.json\", \"w\") as f:\n",
        "    json.dump(log, f)\n",
        "torch.save(model_reg.state_dict(), run_dir / \"model_reg.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ADV evaluated with FGSM\n",
        "We observe that the ADV validation error is lower than the training error because FGSM is a weaker attack thatn PGD\n",
        "Begin adversarial training run: magenta-slow-phoenix-20250105-1507\n",
        "\n",
        "TR      \tTE      \tADV     \tEpoch\n",
        "0.666760\t0.406778\t0.493000\t1\n",
        "0.539120\t0.314444\t0.405000\t2\n",
        "0.476300\t0.274778\t0.379000\t3\n",
        "0.432680\t0.245222\t0.339556\t4\n",
        "0.402620\t0.230444\t0.330000\t5\n",
        "\n",
        "ADV evaluated with PGD-Linf\n",
        "We observe that the ADV validation error is higher than the training error as expected\n",
        "Begin adversarial training run: orange-kind-shark-20250105-1531\n",
        "\n",
        "TR      \tTE      \tADV     \tEpoch\n",
        "0.675220\t0.419667\t0.599000\t1\n",
        "0.543500\t0.310889\t0.508778\t2\n",
        "0.472780\t0.271000\t0.475556\t3\n",
        "0.434380\t0.248000\t0.461889\t4\n",
        "0.403640\t0.230444\t0.457889\t5\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5tjaHMxCFwWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcUht5uBHlYG"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "gold-bright-eagle-20241204-1128:\n",
        "SGD(lr=1e-1), batch_size=1024\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "orange-kind-hawk-20241204-1148:\n",
        "SGD(lr=1e-1, weight_decay=5e-4), batch_size=1024\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "silver-shiny-phoenix-20241204-1210:\n",
        "SGD(lr=1e-1, weight_decay=5e-4, momentum=0.9, nesterov=True), batch_size=1024\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "gray-fierce-octopus-20241204-1226:\n",
        "SGD(lr=1e-1, weight_decay=5e-4, momentum=0.9, nesterov=True), batch_size=1024, CosineAnnealingLR\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "gold-loud-cheetah-20241204-1414:\n",
        "SGD(lr=1e-1, weight_decay=5e-4, momentum=0.9, nesterov=True), batch_size=1024,, epsilon=4/255\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "cyan-wise-eagle-20241203-1710:\n",
        "SGD(lr=1e-1, weight_decay=5e-4, momentum=0.9, nesterov=True), batch_size=1024,, epsilon=32/255\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-ys2JFSidFI5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
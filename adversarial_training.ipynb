{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RubberLanding/AdversarialMachineLearning24/blob/weight_perturbation/adversarial_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preliminaries\n"
      ],
      "metadata": {
        "id": "JN4iX5SEZ1Aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install all needed packages."
      ],
      "metadata": {
        "id": "-bTdL_l2Z6LQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWRfFtr2j9T-",
        "outputId": "a06805ba-d9d2-4998-cb3f-a92a046be254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
            "Collecting git+https://github.com/fra31/auto-attack\n",
            "  Cloning https://github.com/fra31/auto-attack to /tmp/pip-req-build-fr08sspk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/fra31/auto-attack /tmp/pip-req-build-fr08sspk\n",
            "  Resolved https://github.com/fra31/auto-attack to commit a39220048b3c9f2cca9a4d3a54604793c68eca7e\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.4)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.26.4)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.13.1)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install git+https://github.com/fra31/auto-attack\n",
        "!pip install statsmodels\n",
        "# !git pull https://github.com/RubberLanding/AdversarialMachineLearning24"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the weights for the pre-trained Resnet18 on CIFAR-10. We only do this once and store them on a private GoogleDrive, which we later import to be able to actually load the weights."
      ],
      "metadata": {
        "id": "3BunrBffT8NG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KgFph_IRo8BW"
      },
      "outputs": [],
      "source": [
        "# \"\"\"Download pre-trained weights for ResNet18 on CIFAR10\"\"\"\n",
        "# !pip install gdown\n",
        "\n",
        "# # Source: https://github.com/huyvnphan/PyTorch_CIFAR10\n",
        "# FILE_ID = \"17fmN8eQdLpq2jIMQ_X0IXDPXfI9oVWgq\"\n",
        "# file_url = f\"https://drive.google.com/uc?id={FILE_ID}\"\n",
        "\n",
        "# !gdown {file_url}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all packages und functions we need."
      ],
      "metadata": {
        "id": "z95qM1xeZ-g2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DIOCbEGHlAu9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.optim import SGD, Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount your GoogleDrive where you stored the weights for the pre-trained Resnet18 before. We will also store the logging files and model weights there."
      ],
      "metadata": {
        "id": "8KJEoOzHUIjv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX34q7fo-l5R",
        "outputId": "5177ceac-ed79-4107-df4a-57c53e764cfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "project_dir = Path('/content/drive/MyDrive/adversarial_training')\n",
        "project_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "weight_dir = project_dir / \"weights\"\n",
        "weight_dir.mkdir(parents=True, exist_ok=True)\n",
        "weight_file = weight_dir / \"resnet18.pt\"\n",
        "\n",
        "# \"\"\"Extract the pre-trained model weights to Google Drive\"\"\"\n",
        "# with zipfile.ZipFile(\"state_dicts.zip\", \"r\") as zip_ref:\n",
        "#   with zip_ref.open(\"state_dicts/resnet18.pt\") as zf, open(weight_file, 'wb') as f:\n",
        "#       shutil.copyfileobj(zf, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a convenience function to name the different training runs."
      ],
      "metadata": {
        "id": "DYM2QScUaE2z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bqVEbYQFJS_c"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "def generate_run_name():\n",
        "    \"\"\"Generate a random name for a run.\"\"\"\n",
        "    colors = [\n",
        "        \"red\", \"blue\", \"green\", \"yellow\", \"purple\", \"orange\", \"pink\",\n",
        "        \"black\", \"white\", \"gray\", \"silver\", \"gold\", \"cyan\", \"magenta\"]\n",
        "    adjectives = [\n",
        "        \"fast\", \"slow\", \"shiny\", \"dull\", \"bright\", \"dark\", \"silent\",\n",
        "        \"loud\", \"brave\", \"calm\", \"wise\", \"fierce\", \"kind\", \"strong\"]\n",
        "    nouns = [\n",
        "        \"dragon\", \"tiger\", \"lion\", \"panda\", \"wolf\", \"phoenix\", \"eagle\",\n",
        "        \"fox\", \"bear\", \"shark\", \"hawk\", \"cheetah\", \"whale\", \"octopus\"]\n",
        "    color = random.choice(colors)\n",
        "    adjective = random.choice(adjectives)\n",
        "    noun = random.choice(nouns)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "    run_name = f\"{color}-{adjective}-{noun}-{timestamp}\"\n",
        "    return run_name"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "Download the CIFAR-10 data. Split it into training, validation and test set. Do some pre-processing."
      ],
      "metadata": {
        "id": "HgXUQL_saQG7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLo5cWwgkquv",
        "outputId": "0235b677-0a2b-4831-e052-03b01e4a7a01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "batch_size = 1024 # batch size has to be < 2**16, should be <= 2**13 for T4\n",
        "debug = True\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.491, 0.482, 0.446], std=[0.247, 0.243, 0.261]),\n",
        "    ])\n",
        "\n",
        "\"\"\" Load data \"\"\"\n",
        "data_train = CIFAR10(root=\"datasets\", train=True, download=True, transform=transform)\n",
        "data_test = CIFAR10(root=\"datasets\", train=False, download=True, transform=transform)\n",
        "data_test, data_val = torch.utils.data.random_split(data_test, [0.1, 0.9])\n",
        "\n",
        "dataloader_train = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
        "dataloader_test = DataLoader(data_test, batch_size=len(data_test), shuffle=False) # create test dataloader with a single batch\n",
        "dataloader_val = DataLoader(data_val, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "num_classes = len(data_train.classes)\n",
        "\n",
        "# mean = data_train.data.mean(axis=(0,1,2)) / 255 # [0.49139968, 0.48215841, 0.44653091]\n",
        "# std = data_train.data.std(axis=(0,1,2)) / 255 # [0.24703223, 0.24348513, 0.26158784]\n",
        "\n",
        "data_train_subset = Subset(data_train, list(range(2*batch_size)))\n",
        "data_val_subset = Subset(data_val, list(range(10)))\n",
        "data_test_subset = Subset(data_test, list(range(100)))\n",
        "\n",
        "dataloader_train_subset = DataLoader(data_train_subset, batch_size=batch_size, shuffle=True)\n",
        "dataloader_val_subset = DataLoader(data_val_subset, batch_size=len(data_val_subset), shuffle=False)\n",
        "dataloader_test_subset = DataLoader(data_test_subset, batch_size=len(data_test_subset), shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attacks\n",
        "Define the attacks."
      ],
      "metadata": {
        "id": "TLTLCMphadmb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GSHC7WdoyxSC"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def fgsm(model, X, y, epsilon=8/255):\n",
        "    \"\"\" Construct FGSM adversarial examples on the examples X\"\"\"\n",
        "    delta = torch.zeros_like(X, requires_grad=True)\n",
        "    loss = CrossEntropyLoss()(model(X + delta), y)\n",
        "    loss.backward()\n",
        "    return epsilon * delta.grad.detach().sign()\n",
        "\n",
        "def pgd_linf(model, X, y, epsilon=8/255, alpha=2/255, num_iter=10, randomize=False):\n",
        "    \"\"\" Construct FGSM adversarial examples on the examples X\"\"\"\n",
        "    if randomize:\n",
        "        delta = torch.rand_like(X, requires_grad=True)\n",
        "        delta.data = delta.data * 2 * epsilon - epsilon\n",
        "    else:\n",
        "        delta = torch.zeros_like(X, requires_grad=True)\n",
        "\n",
        "    for t in range(num_iter):\n",
        "        loss = CrossEntropyLoss()(model(X + delta), y)\n",
        "        loss.backward()\n",
        "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
        "        delta.grad.zero_()\n",
        "    return delta.detach()\n",
        "\n",
        "def pgd_linf_trades(model, X, y, epsilon=8/255, alpha=2/255, num_iter=10, randomize=False):\n",
        "    \"\"\" Construct FGSM adversarial examples with KL-Divergence for TRADES.\n",
        "        Should be used together with traded_loss().\n",
        "    \"\"\"\n",
        "    if randomize:\n",
        "        delta = torch.rand_like(X, requires_grad=True)\n",
        "        delta.data = delta.data * 2 * epsilon - epsilon\n",
        "    else:\n",
        "        delta = torch.zeros_like(X, requires_grad=True)\n",
        "\n",
        "    for t in range(num_iter):\n",
        "        # maybe set log_target=True and pass F.log_softmax(model(X), dim=1),\n",
        "        # see the docs for more details\n",
        "        loss = F.kl_div(F.log_softmax(model(X + delta), dim=1),\n",
        "                        F.softmax(model(X), dim=1),\n",
        "                        reduction='batchmean')\n",
        "        loss.backward()\n",
        "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
        "        delta.grad.zero_()\n",
        "    return delta.detach()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Losses\n",
        "We define a wrapper, that calls the appropriate loss function with the correct arguments. Inside the wrapper, we implement the loss functions."
      ],
      "metadata": {
        "id": "3cPBOyOCkiLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LossWrapper:\n",
        "    def __init__(self, loss_fn, lambda_tradeoff=1.0):\n",
        "        self.loss_fn = loss_fn\n",
        "        self.lambda_tradeoff = lambda_tradeoff\n",
        "\n",
        "    def __call__(self, model, X, y, delta=0.0):\n",
        "        \"\"\"Args:\n",
        "              model: The model being trained.\n",
        "              X: Clean input data.\n",
        "              delta: Perturbations applied to X.\n",
        "              y: Ground-truth labels.\n",
        "        \"\"\"\n",
        "        # Cross-Entropy Loss\n",
        "        if self.loss_fn == \"CE\":\n",
        "            yp = model(X + delta)\n",
        "            return F.cross_entropy(yp, y)\n",
        "\n",
        "        # TRADES Loss\n",
        "        elif self.loss_fn == \"TRADES\":\n",
        "            yp_adv = model(X + delta)\n",
        "            yp_clean = model(X)\n",
        "            clean_loss = F.cross_entropy(yp_clean, y)\n",
        "            robust_loss = F.kl_div(F.log_softmax(yp_adv, dim=1),\n",
        "                            F.softmax(yp_clean, dim=1),\n",
        "                            reduction='batchmean')\n",
        "            return clean_loss + self.lambda_tradeoff * robust_loss\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported loss function\")"
      ],
      "metadata": {
        "id": "9XMbYBVLkkWr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "rSjV2ajnb_Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define functions that train a model."
      ],
      "metadata": {
        "id": "K5-e2T8AcHZV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Dxm_8JdYoK3D"
      },
      "outputs": [],
      "source": [
        "def train_epoch(loader, model, opt, loss_fn):\n",
        "    \"\"\"Standard training/evaluation epoch over the dataset\"\"\"\n",
        "    model.train()\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        yp = model(X)\n",
        "        loss = loss_fn(model, X, y)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(loader, model, loss_fn):\n",
        "    \"\"\"Standard training/evaluation epoch over the dataset\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        yp = model(X)\n",
        "        loss = loss_fn(model, X, y)\n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n",
        "def train_epoch_adversarial(loader, model, attack, opt, loss_fn, **kwargs):\n",
        "    \"\"\"Adversarial training/evaluation epoch over the dataset\"\"\"\n",
        "    model.train()\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        delta = attack(model, X, y, **kwargs)\n",
        "        yp = model(X+delta)\n",
        "        loss = loss_fn(model, X, y, delta)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n",
        "def eval_epoch_adversarial(loader, model, attack, loss_fn, **kwargs):\n",
        "    \"\"\"Adversarial training/evaluation epoch over the dataset\"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss, total_err = 0.0, 0.0\n",
        "\n",
        "    for X, y in loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute adversarial perturbations (requires gradients)\n",
        "        with torch.enable_grad():\n",
        "            delta = attack(model, X, y, **kwargs)\n",
        "\n",
        "        # Evaluate the model on adversarial examples without gradients\n",
        "        with torch.no_grad():\n",
        "            yp = model(X + delta)\n",
        "            loss = loss_fn(model, X, y, delta)\n",
        "\n",
        "            total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "            total_loss += loss.item() * X.shape[0]\n",
        "\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n",
        "def train_epoch_rs(loader, model, opt, loss_fn, sigma=0.25):\n",
        "    \"\"\"Training epoch with Gaussian noise over the dataset for Randomized Smoothing\"\"\"\n",
        "    model.train()\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        yp = model(X + torch.randn_like(X) * sigma) # Add Gaussian noise\n",
        "        loss = loss_fn(model, X, y)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch_rs(loader, model, loss_fn, sigma=0.25):\n",
        "    \"\"\"Standard training/evaluation epoch over the dataset\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        yp = model(X + torch.randn_like(X) * sigma)\n",
        "        loss = loss_fn(model, X, y)\n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
        "\n",
        "def train_epoch_awp(loader, model, attack, opt, loss_fn, awp, **kwargs):\n",
        "    \"\"\"Adversarial Weight Perturbation training/evaluation epoch over the dataset\"\"\"\n",
        "    model.train()\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "\n",
        "        #Perform AWP attack before forwad pass\n",
        "        awp.attack_backward(X, y) # Apply perturbation\n",
        "\n",
        "        delta = attack(model, X, y, **kwargs)\n",
        "        yp = model(X+delta)\n",
        "        loss = loss_fn(model, X, y, delta)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "zqdzqrZxNRYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial Weight Perturbation\n",
        "\n",
        "Instead of perturbing on the input-loss landscape, this method perturbs the weight space during training, making it more robust to adversarial gradients. As a consequence the loss landscape becomes smoother. This makes it harder for PGD and FGSM types of attacks to find steepness in the loss landscape."
      ],
      "metadata": {
        "id": "r6M3gkuvPtmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the Model\n"
      ],
      "metadata": {
        "id": "e08YzxUnP5y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from torch.nn import CrossEntropyLoss, Conv2d\n",
        "\n",
        "###########################\n",
        "# LOAD THE MODEL\n",
        "\n",
        "model_adv = resnet18()\n",
        "\n",
        "# CIFAR10: kernel_size 7 -> 3, stride 2 -> 1, padding 3->1\n",
        "model_adv.conv1 = Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_adv.fc = torch.nn.Linear(model_adv.fc.in_features, num_classes)\n",
        "\n",
        "pretrained_weights = torch.load(weight_file, weights_only=True)\n",
        "model_adv.load_state_dict(pretrained_weights)\n",
        "model_adv = model_adv.to(device)\n",
        "\n",
        "###########################\n",
        "# SET LOGGING\n",
        "\n",
        "run_dir = project_dir / 'adversarial_weight_perturbation_with_trades'\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "log = {key: [] for key in [\"train_losses\", \"test_losses\", \"adv_losses\",\n",
        "                           \"train_errors\", \"test_errors\", \"adv_errors\"]}\n"
      ],
      "metadata": {
        "id": "DD8J4ztwP1S0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adversarial Weight Perturbation functions\n",
        "\n",
        "We define a function that takes a model and a datapoint, computes the weights for that data points and then perturbs it with a magnitude of epsilon and the sign of the gradient to maximise the loss. There is an inner optimization loop that finds the worst-case weight perturbation. The number of steps is a hyperparameter.\n",
        "\n",
        "We also define training and validation functions. We can use these functions to integrate AWP into a regular adversarial training loop."
      ],
      "metadata": {
        "id": "AqT1HkTeQOGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AWP(object):\n",
        "    def __init__(self, model, optimizer, adv_param=\"weight\", adv_lr=0.01, adv_eps=5e-3):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.adv_param = adv_param\n",
        "        self.adv_lr = adv_lr\n",
        "        self.adv_eps = adv_eps\n",
        "        self.backup = {}\n",
        "        self.grad_backup = {}\n",
        "\n",
        "    def attack_backward(self, x, y):\n",
        "        \"\"\"\n",
        "        This is the function to call to execute the attack.\n",
        "        It will update the model weights in-place.\n",
        "        \"\"\"\n",
        "        if self.adv_param == \"weight\":\n",
        "            self._save()  # Save model weights before perturbation\n",
        "            self._attack_step()  # Perform adversarial weight perturbation\n",
        "        elif self.adv_param == \"gradient\":\n",
        "            self._attack_step_grad()  # Perform adversarial gradient perturbation\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid adv_param: {self.adv_param}\")\n",
        "\n",
        "        # Calculate loss with perturbed weights\n",
        "        loss = F.cross_entropy(self.model(x), y)\n",
        "\n",
        "        # Update model parameters\n",
        "        self.optimizer.zero_grad()  # Reset gradients before backward pass\n",
        "        loss.backward()\n",
        "        self.optimizer.step()  # Update model weights\n",
        "\n",
        "        if self.adv_param == \"weight\":\n",
        "            self._restore()  # Restore original model weights\n",
        "\n",
        "    def _attack_step(self):\n",
        "        e = 1e-6\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
        "                norm1 = torch.norm(param.grad)\n",
        "                norm2 = torch.norm(param.data.detach())\n",
        "                if norm1 != 0 and norm2 != 0:\n",
        "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
        "                    param.data.add_(r_at)\n",
        "                    param.data = torch.min(\n",
        "                        torch.max(param.data, self.backup[name] - self.adv_eps),\n",
        "                        self.backup[name] + self.adv_eps,\n",
        "                    )\n",
        "\n",
        "    def _attack_step_grad(self):\n",
        "        e = 1e-6\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
        "                # Compute the adversarial gradient perturbation\n",
        "                # Here, it's a simple scaled version of the gradient\n",
        "                r_at = self.adv_lr * param.grad\n",
        "\n",
        "                # Add the perturbation to the gradient\n",
        "                param.grad.add_(r_at)\n",
        "\n",
        "                # Optional: Clipping the gradient\n",
        "                # torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "    def _save(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
        "                if name not in self.backup:\n",
        "                    self.backup[name] = param.data.clone()\n",
        "                    self.grad_backup[name] = param.grad.clone()\n",
        "\n",
        "    def _restore(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if name in self.backup:\n",
        "                param.data.copy_(self.backup[name])"
      ],
      "metadata": {
        "id": "0HbNid8-QJep"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Adversarial Weight Perturbation model"
      ],
      "metadata": {
        "id": "uZtgFoLgUHpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam, SGD\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
        "\n",
        "opt = Adam(model_adv.parameters(), lr=1e-3, weight_decay=5e-4)\n",
        "# opt = SGD(model_adv.parameters(), lr=1e-2, momentum=0.9, weight_decay=5e-4,nesterov=False)\n",
        "\n",
        "# scheduler = CosineAnnealingWarmRestarts(opt, T_0=10, T_mult=2, eta_min=0)\n",
        "epochs = 200\n",
        "scheduler = CosineAnnealingLR(opt, T_max=epochs,eta_min=1e-6)\n",
        "epsilon = 5e-3\n",
        "num_steps = 1\n",
        "trades = LossWrapper(\"TRADES\")\n",
        "cross_entropy = LossWrapper(\"CE\")\n",
        "\n",
        "# Create instance of AWP class\n",
        "awp = AWP(model_adv,opt)\n",
        "\n",
        "log = {key: [] for key in [\"train_losses\", \"test_losses\", \"adv_losses\",\n",
        "                           \"train_errors\", \"test_errors\", \"adv_errors\"]}\n",
        "\n",
        "print(f\"Begin adversarial training run: {run_dir.stem}\\n\")\n",
        "print(*(\"TR      \", \"TE      \", \"ADV     \", \"Epoch   \"), sep=\"\\t\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_err, train_loss = train_epoch_awp(dataloader_train, model_adv, pgd_linf_trades, loss_fn=trades, awp=awp, opt=opt)\n",
        "  test_err, test_loss = eval_epoch(dataloader_val, model_adv, loss_fn=cross_entropy)\n",
        "  adv_err, adv_loss = eval_epoch_adversarial(dataloader_val, model_adv, fgsm, loss_fn=cross_entropy)\n",
        "\n",
        "  scheduler.step()\n",
        "  log[\"train_losses\"] += [train_loss]\n",
        "  log[\"test_losses\"] += [test_loss]\n",
        "  log[\"adv_losses\"] += [adv_loss]\n",
        "  log[\"train_errors\"] += [train_err]\n",
        "  log[\"test_errors\"] += [test_err]\n",
        "  log[\"adv_errors\"] += [adv_err]\n",
        "\n",
        "  print(*(\"{:.6f}\".format(train_err),\n",
        "          \"{:.6f}\".format(test_err),\n",
        "          \"{:.6f}\".format(adv_err),\n",
        "          f\"{epoch+1}\",), sep=\"\\t\")\n",
        "\n",
        "  ###########################\n",
        "# STORE RESULTS\n",
        "store = True # set this variable to True when you have runs that you want to save\n",
        "if store:\n",
        "  with open(run_dir / \"log.json\", \"w\") as f:\n",
        "      json.dump(log, f)\n",
        "  torch.save(model_adv.state_dict(), run_dir / \"model_adv.pt\")\n"
      ],
      "metadata": {
        "id": "hrWeQ8SsUGxd",
        "outputId": "46351928-19ca-4ce1-cd13-a96c6557b127",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin adversarial training run: adversarial_weight_perturbation_with_trades\n",
            "\n",
            "TR      \tTE      \tADV     \tEpoch   \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ea44e7caf656>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mtrain_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch_awp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpgd_linf_trades\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrades\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mawp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mawp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0mtest_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0madv_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_epoch_adversarial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfgsm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-1daf593516fe>\u001b[0m in \u001b[0;36mtrain_epoch_awp\u001b[0;34m(loader, model, attack, opt, loss_fn, awp, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mawp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Apply perturbation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0myp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-8893d3b1cebc>\u001b[0m in \u001b[0;36mpgd_linf_trades\u001b[0;34m(model, X, y, epsilon, alpha, num_iter, randomize)\u001b[0m\n\u001b[1;32m     39\u001b[0m                         \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                         reduction='batchmean')\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the Advarsarial Training Model\n"
      ],
      "metadata": {
        "id": "oRmrIIaWd4rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from torch.nn import CrossEntropyLoss, Conv2d\n",
        "\n",
        "###########################\n",
        "# LOAD THE MODEL\n",
        "\n",
        "model_adv = resnet18()\n",
        "\n",
        "# CIFAR10: kernel_size 7 -> 3, stride 2 -> 1, padding 3->1\n",
        "model_adv.conv1 = Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_adv.fc = torch.nn.Linear(model_adv.fc.in_features, num_classes)\n",
        "\n",
        "pretrained_weights = torch.load(weight_file, weights_only=True)\n",
        "model_adv.load_state_dict(pretrained_weights)\n",
        "model_adv = model_adv.to(device)\n",
        "\n",
        "###########################\n",
        "# SET LOGGING\n",
        "\n",
        "run_dir = project_dir / generate_run_name()\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "log = {key: [] for key in [\"train_losses\", \"test_losses\", \"adv_losses\",\n",
        "                           \"train_errors\", \"test_errors\", \"adv_errors\"]}\n"
      ],
      "metadata": {
        "id": "hLdE6jITd8PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Adversarial Training Model\n",
        "Use adversarial training to train the robust model."
      ],
      "metadata": {
        "id": "xh7mVivIZqA6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNAZgohWXLfM",
        "outputId": "95420750-4dbf-4305-acc7-2311587b9091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin adversarial training run: orange-fierce-shark-20250110-1652\n",
            "\n",
            "TR      \tTE      \tADV     \tEpoch   \n",
            "0.626953\t0.881836\t0.900391\t1\n",
            "0.663086\t0.654297\t0.766602\t2\n",
            "0.527832\t0.529297\t0.693359\t3\n",
            "0.488281\t0.471680\t0.630859\t4\n",
            "0.428223\t0.421875\t0.595703\t5\n",
            "0.390625\t0.392578\t0.596680\t6\n",
            "0.351074\t0.377930\t0.587891\t7\n",
            "0.308105\t0.366211\t0.580078\t8\n",
            "0.262695\t0.351562\t0.568359\t9\n",
            "0.233887\t0.329102\t0.559570\t10\n",
            "0.197266\t0.332031\t0.552734\t11\n",
            "0.170898\t0.321289\t0.539062\t12\n",
            "0.141113\t0.310547\t0.532227\t13\n",
            "0.111328\t0.318359\t0.533203\t14\n",
            "0.080566\t0.323242\t0.543945\t15\n"
          ]
        }
      ],
      "source": [
        "###########################\n",
        "# SET TRAINING PARAMETERS\n",
        "\n",
        "opt = Adam(model_adv.parameters(), lr=1e-3)\n",
        "# opt = SGD(model_adv.parameters(), lr=1e-1)\n",
        "# scheduler = CosineAnnealingLR(opt, T_max=100)\n",
        "# scheduler = CosineAnnealingWarmRestarts(opt, T_0=10, T_mult=2, eta_min=0)\n",
        "\n",
        "epochs = 15\n",
        "\n",
        "trades = LossWrapper(\"TRADES\")\n",
        "cross_entropy = LossWrapper(\"CE\")\n",
        "###########################\n",
        "# START TRAINING\n",
        "\n",
        "print(f\"Begin adversarial training run: {run_dir.stem}\\n\")\n",
        "print(*(\"TR      \", \"TE      \", \"ADV     \", \"Epoch   \"), sep=\"\\t\")\n",
        "\n",
        "for t in range(epochs):\n",
        "    train_err, train_loss = train_epoch_adversarial(dataloader_train_subset, model_adv, pgd_linf_trades, opt, loss_fn=trades)\n",
        "    test_err, test_loss = eval_epoch(dataloader_val_subset, model_adv, loss_fn=cross_entropy)\n",
        "    adv_err, adv_loss = eval_epoch_adversarial(dataloader_val_subset, model_adv, fgsm, loss_fn=cross_entropy)\n",
        "\n",
        "    # Update the losses and errors\n",
        "    log[\"train_losses\"] += [train_loss]\n",
        "    log[\"test_losses\"] += [test_loss]\n",
        "    log[\"adv_losses\"] += [adv_loss]\n",
        "    log[\"train_errors\"] += [train_err]\n",
        "    log[\"test_errors\"] += [test_err]\n",
        "    log[\"adv_errors\"] += [adv_err]\n",
        "\n",
        "    print(*(\"{:.6f}\".format(train_err),\n",
        "            \"{:.6f}\".format(test_err),\n",
        "            \"{:.6f}\".format(adv_err),\n",
        "            f\"{t+1}\",), sep=\"\\t\")\n",
        "\n",
        "###########################\n",
        "# STORE RESULTS\n",
        "store = False # set this variable to True when you have runs that you want to save\n",
        "if store:\n",
        "  with open(run_dir / \"log.json\", \"w\") as f:\n",
        "      json.dump(log, f)\n",
        "  torch.save(model_adv.state_dict(), run_dir / \"model_adv.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install AutoAttack to calculate a robust accuracy for the model to get a fair comparison, e.g. with models from [RobustBench](https://robustbench.github.io/)."
      ],
      "metadata": {
        "id": "n0Y3Q_ZlZy3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/fra31/auto-attack"
      ],
      "metadata": {
        "id": "3CJs5EBFu26D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autoattack import AutoAttack\n",
        "\n",
        "X, y = next(iter(dataloader_test)) # optimally, the test loader has a single batch containing the entire test set\n",
        "X, y = X.to(device), y.to(device)\n",
        "\n",
        "adversary = AutoAttack(model_adv, norm='Linf', eps=8/255, version='standard')\n",
        "adv_examples = adversary.run_standard_evaluation(X, y, bs=batch_size) # this takes ~8min on T4 on a single test batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uknpMFbi73zO",
        "outputId": "c1c7d7a6-5fc1-4fb7-a49c-c8dd79c84c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "setting parameters for standard version\n",
            "using standard version including apgd-ce, apgd-t, fab-t, square.\n",
            "initial accuracy: 58.30%\n",
            "apgd-ce - 1/5 - 35 out of 128 successfully perturbed\n",
            "apgd-ce - 2/5 - 29 out of 128 successfully perturbed\n",
            "apgd-ce - 3/5 - 32 out of 128 successfully perturbed\n",
            "apgd-ce - 4/5 - 31 out of 128 successfully perturbed\n",
            "apgd-ce - 5/5 - 23 out of 71 successfully perturbed\n",
            "robust accuracy after APGD-CE: 43.30% (total time 16.5 s)\n",
            "apgd-t - 1/4 - 17 out of 128 successfully perturbed\n",
            "apgd-t - 2/4 - 14 out of 128 successfully perturbed\n",
            "apgd-t - 3/4 - 16 out of 128 successfully perturbed\n",
            "apgd-t - 4/4 - 7 out of 49 successfully perturbed\n",
            "robust accuracy after APGD-T: 37.90% (total time 122.8 s)\n",
            "fab-t - 1/3 - 3 out of 128 successfully perturbed\n",
            "fab-t - 2/3 - 2 out of 128 successfully perturbed\n",
            "fab-t - 3/3 - 2 out of 123 successfully perturbed\n",
            "robust accuracy after FAB-T: 37.20% (total time 285.0 s)\n",
            "square - 1/3 - 1 out of 128 successfully perturbed\n",
            "square - 2/3 - 1 out of 128 successfully perturbed\n",
            "square - 3/3 - 2 out of 116 successfully perturbed\n",
            "robust accuracy after SQUARE: 36.80% (total time 446.3 s)\n",
            "Warning: Square Attack has decreased the robust accuracy of 0.40%. This might indicate that the robustness evaluation using AutoAttack is unreliable. Consider running Square Attack with more iterations and restarts or an adaptive attack. See flags_doc.md for details.\n",
            "max Linf perturbation: 1.98785, nan in tensor: 0, max: 2.13169, min: -1.98785\n",
            "robust accuracy: 36.80%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the Weight Averaging Model\n",
        "Here we initialize an exponential moving average (EMA) model, based on the robust model."
      ],
      "metadata": {
        "id": "O32SQzimNaFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from torch.nn import CrossEntropyLoss, Conv2d\n",
        "\n",
        "###########################\n",
        "# LOAD THE MODEL\n",
        "\n",
        "model_adv = resnet18()\n",
        "\n",
        "# CIFAR10: kernel_size 7 -> 3, stride 2 -> 1, padding 3->1\n",
        "model_adv.conv1 = Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_adv.fc = torch.nn.Linear(model_adv.fc.in_features, num_classes)\n",
        "\n",
        "pretrained_weights = torch.load(weight_file, weights_only=True)\n",
        "model_adv.load_state_dict(pretrained_weights)\n",
        "\n",
        "model_ema = torch.optim.swa_utils.AveragedModel(model_adv, multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.9))\n",
        "\n",
        "model_adv = model_adv.to(device)\n",
        "model_ema = model_ema.to(device)\n",
        "\n",
        "###########################\n",
        "# SET LOGGING\n",
        "\n",
        "run_dir = project_dir / generate_run_name()\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "log = {key: [] for key in [\"train_losses\", \"test_losses\", \"adv_losses\",\n",
        "                           \"train_errors\", \"test_errors\", \"adv_errors\"]}\n"
      ],
      "metadata": {
        "id": "3Ed0KCUxMdQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FML825DiaHuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Weight Averaged Model"
      ],
      "metadata": {
        "id": "FEf0SlFQaN0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# SET TRAINING PARAMETERS\n",
        "\n",
        "opt = Adam(model_adv.parameters(), lr=1e-3)\n",
        "# opt = SGD(model_adv.parameters(), lr=1e-1)\n",
        "# scheduler = CosineAnnealingLR(opt, T_max=100)\n",
        "# scheduler = CosineAnnealingWarmRestarts(opt, T_0=10, T_mult=2, eta_min=0)\n",
        "\n",
        "epochs = 15\n",
        "\n",
        "trades = LossWrapper(\"TRADES\")\n",
        "cross_entropy = LossWrapper(\"CE\")\n",
        "\n",
        "###########################\n",
        "# START TRAINING\n",
        "\n",
        "print(f\"Begin adversarial training run: {run_dir.stem}\\n\")\n",
        "print(*(\"TR      \", \"TE      \", \"ADV     \", \"Epoch   \"), sep=\"\\t\")\n",
        "\n",
        "for t in range(epochs):\n",
        "    train_err, train_loss = train_epoch_adversarial(dataloader_train_subset, model_adv, pgd_linf, opt, loss_fn=cross_entropy)\n",
        "    model_ema.update_parameters(model_adv) # Update EMA model\n",
        "\n",
        "    test_err, test_loss = eval_epoch(dataloader_val_subset, model_ema, loss_fn=cross_entropy) # Evaluate clean acc. on EMA model\n",
        "    adv_err, adv_loss = eval_epoch_adversarial(dataloader_val_subset, model_ema, fgsm, loss_fn=cross_entropy) # Evaluate robust acc. on EMA model\n",
        "\n",
        "    # Update the losses and errors\n",
        "    log[\"train_losses\"] += [train_loss]\n",
        "    log[\"test_losses\"] += [test_loss]\n",
        "    log[\"adv_losses\"] += [adv_loss]\n",
        "    log[\"train_errors\"] += [train_err]\n",
        "    log[\"test_errors\"] += [test_err]\n",
        "    log[\"adv_errors\"] += [adv_err]\n",
        "\n",
        "    print(*(\"{:.6f}\".format(train_err),\n",
        "            \"{:.6f}\".format(test_err),\n",
        "            \"{:.6f}\".format(adv_err),\n",
        "            f\"{t+1}\",), sep=\"\\t\")\n",
        "\n",
        "###########################\n",
        "# STORE RESULTS\n",
        "store = False # set this variable to True when you have runs that you want to save\n",
        "if store:\n",
        "  with open(run_dir / \"log.json\", \"w\") as f:\n",
        "      json.dump(log, f)\n",
        "  torch.save(model_adv.state_dict(), run_dir / \"model_adv.pt\")\n",
        "  torch.save(model_ema.state_dict(), run_dir / \"model_ema.pt\")"
      ],
      "metadata": {
        "id": "a52C46yiN7Y9",
        "outputId": "da6f6df0-1aa5-42af-9af1-588ac9af7218",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin adversarial training run: gray-loud-fox-20250110-1808\n",
            "\n",
            "TR      \tTE      \tADV     \tEpoch   \n",
            "0.732910\t0.805664\t0.852539\t1\n",
            "0.758301\t0.713867\t0.785156\t2\n",
            "0.650391\t0.889648\t0.894531\t3\n",
            "0.620605\t0.897461\t0.898438\t4\n",
            "0.579102\t0.893555\t0.895508\t5\n",
            "0.544922\t0.886719\t0.890625\t6\n",
            "0.514648\t0.884766\t0.887695\t7\n",
            "0.481934\t0.871094\t0.879883\t8\n",
            "0.457031\t0.828125\t0.862305\t9\n",
            "0.412598\t0.760742\t0.809570\t10\n",
            "0.385742\t0.684570\t0.737305\t11\n",
            "0.358398\t0.646484\t0.712891\t12\n",
            "0.323730\t0.648438\t0.708008\t13\n",
            "0.275879\t0.602539\t0.669922\t14\n",
            "0.239746\t0.542969\t0.632812\t15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Randomized Smoothing"
      ],
      "metadata": {
        "id": "DfLDKI1QwgeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a RandomizerSmoother. This class takes a base model and uses it to perform Randomized Smoothing. In theory, this allows to get certified robustness guarantees for any classifier.   "
      ],
      "metadata": {
        "id": "0180fxuQgEW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.proportion import proportion_confint\n",
        "from scipy.stats import norm, binomtest\n",
        "\n",
        "class RandomizedSmoother(object):\n",
        "\n",
        "  def __init__(self, base_model, num_classes, sigma=0.25, epsilon=8/255, alpha=0.05, norm=\"L2\"):\n",
        "    self.base_model = base_model\n",
        "    self.num_classes = num_classes\n",
        "    self.sigma = sigma\n",
        "    self.epsilon = epsilon\n",
        "    self.norm = norm\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def predict(self, X, num_samples=100):\n",
        "    batch_size = X.size(0)\n",
        "    # Compute class counts for noisy samples\n",
        "    class_counts = self.get_noisy_counts(X, num_samples)\n",
        "\n",
        "    # Sort class counts to find top 2 most frequent classes\n",
        "    sorted_counts_vals, sorted_counts_ind = class_counts.sort()\n",
        "    top1_class = sorted_counts_ind[:, -1]  # Most frequent class\n",
        "    top2_class = sorted_counts_ind[:, -2]  # Second most frequent class\n",
        "    top1_class_count = sorted_counts_vals[:, -1]\n",
        "    top2_class_count = sorted_counts_vals[:, -2]\n",
        "\n",
        "    predictions = torch.zeros(batch_size, device=X.device)\n",
        "    for i in range(batch_size):\n",
        "        # Use binomial test to check if the top class is significantly more frequent\n",
        "        n_a, n_b = top1_class_count[i], top2_class_count[i]\n",
        "        predictions[i] = top1_class[i] if binomtest(n_a, n_a + n_b, p=0.5).pvalue <= self.alpha else float('nan')\n",
        "\n",
        "    return predictions\n",
        "\n",
        "  def certify(self, X, num_samples_selection=100, num_samples_estimation=100):\n",
        "    batch_size = X.size(0)\n",
        "    # Perform two sampling procedures to avoid selection bias\n",
        "    counts_a = self.get_noisy_counts(X, num_samples_selection)\n",
        "    top1_class = counts_a.argmax(dim=-1).unsqueeze(1)  # Get most frequent class index\n",
        "\n",
        "    counts = self.get_noisy_counts(X, num_samples_estimation)\n",
        "    # Extract the counts for the most frequent class\n",
        "    n_a = counts.gather(dim=1, index=top1_class).squeeze()\n",
        "\n",
        "    certified_radii = torch.zeros(batch_size)\n",
        "    predictions = torch.zeros(batch_size, device=X.device)\n",
        "    for i in range(batch_size):\n",
        "        # Compute lower confidence bound for the top class probability\n",
        "        conf_bound, _ = proportion_confint(n_a[i], num_samples_estimation, alpha=2 * self.alpha, method=\"beta\")\n",
        "        # Calculate the certified radius based on the confidence bound\n",
        "        if conf_bound >= 0.5:\n",
        "          certified_radii[i] = self.get_radius(conf_bound, self.norm)\n",
        "          predictions[i] = top1_class[i]\n",
        "        else:\n",
        "          certified_radii[i] = float(\"nan\")\n",
        "          predictions[i] = float(\"nan\")\n",
        "\n",
        "    return certified_radii, predictions\n",
        "\n",
        "  def get_noisy_counts(self, X, num_samples):\n",
        "    batch_size = X.size(0)\n",
        "    class_counts = torch.zeros(batch_size, self.num_classes, dtype=torch.int64, device=X.device)\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # Generate noisy samples based on the specified norm\n",
        "        noisy_samples = X + self.get_noise(X.shape, self.norm)\n",
        "        with torch.no_grad():\n",
        "            # Predict classes for noisy samples\n",
        "            logits = self.base_model(noisy_samples)\n",
        "            class_pred = F.softmax(logits, dim=-1).argmax(dim=-1)\n",
        "            # Update class counts\n",
        "            for i in range(batch_size):\n",
        "                class_counts[i, class_pred[i]] += 1\n",
        "\n",
        "    return class_counts\n",
        "\n",
        "  def get_noise(self, shape, p_norm=\"L2\"):\n",
        "    if p_norm==\"L2\":\n",
        "      return torch.randn(shape) * self.sigma\n",
        "    elif p_norm==\"Linf\":\n",
        "      return 2 * self.epsilon * torch.rand(shape) - self.epsilon\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported norm.\")\n",
        "\n",
        "  def get_radius(self, conf_bound, p_norm=\"L2\"):\n",
        "    if p_norm==\"L2\":\n",
        "      return self.sigma * norm.ppf(conf_bound)\n",
        "    elif p_norm==\"Linf\":\n",
        "      return self.epsilon * (2 * conf_bound - 1)\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported norm.\")"
      ],
      "metadata": {
        "id": "9LUFmmFsxTyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the regular model with pre-trained weights and a model that was adversarially trained with the TRADES loss."
      ],
      "metadata": {
        "id": "N7y0lNU8gi_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from torch.nn import CrossEntropyLoss, Conv2d\n",
        "\n",
        "###########################\n",
        "# LOAD THE MODEL\n",
        "\n",
        "def get_resnet18():\n",
        "  model = resnet18()\n",
        "  # CIFAR10: kernel_size 7 -> 3, stride 2 -> 1, padding 3->1\n",
        "  model.conv1 = Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "  model.fc = torch.nn.Linear(model.fc.in_features, 10)\n",
        "  return model\n",
        "\n",
        "model_base = get_resnet18()\n",
        "pretrained_weights = torch.load(weight_file, weights_only=True)\n",
        "model_base.load_state_dict(pretrained_weights)\n",
        "model_base = model_base.to(device)\n",
        "\n",
        "model_trades = get_resnet18()\n",
        "pretrained_weights_trades = torch.load(weight_dir / \"resnet18_trades.pt\", weights_only=True, map_location=torch.device('cpu'))\n",
        "model_trades.load_state_dict(pretrained_weights_trades)\n",
        "model_trades = model_trades.to(device)\n",
        "\n",
        "X, y = next(iter(dataloader_val_subset)) # Loader should be single batch containing all the data\n",
        "X, y = X.to(device), y.to(device)"
      ],
      "metadata": {
        "id": "8qVgeFAQwf0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a function that calculates the certified accuracy in dependence on the radius. This means, that for each sample in the dataset, we check whether its radius is higher than the reference value for the radius. If so, we include it in our calculation for the accuracy."
      ],
      "metadata": {
        "id": "BWWBOt9-g-BJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.set_printoptions(threshold=100)\n",
        "\n",
        "def calculate_certified_acc(radius, yp, y, num_bins=1000):\n",
        "  num_samples = len(y)\n",
        "  ind_not_nan = ~torch.isnan(radius)\n",
        "  radius, yp, y = radius[ind_not_nan], yp[ind_not_nan], y[ind_not_nan] # Filter out entries with NaN\n",
        "  max_val = radius.max()\n",
        "  bins = torch.linspace(0.0, max_val, num_bins + 1)\n",
        "  counts = np.array([(yp[radius > bin_value] == y[radius > bin_value]).sum() for bin_value in bins])\n",
        "  acc = counts / num_samples\n",
        "\n",
        "  return bins, acc\n",
        "\n",
        "def plot_certified_acc(model_base, X, y, num_classes=10, epsilon=8/255, alpha=0.05):\n",
        "  num_samples_estimation = 200\n",
        "  num_samples_selection = 100\n",
        "  sigmas = [0.1, 0.25, 0.5]\n",
        "\n",
        "  n_sigma = len(sigmas)\n",
        "  cmap = plt.get_cmap(\"viridis\", n_sigma)\n",
        "  colors = [cmap(i) for i in range(n_sigma)]\n",
        "\n",
        "  for i, sigma in enumerate(sigmas):\n",
        "    color = colors[i]\n",
        "    model_rs = RandomizedSmoother(model_base, num_classes, sigma, epsilon, alpha)\n",
        "    radius, yp = model_rs.certify(X, num_samples_selection, num_samples_estimation)\n",
        "\n",
        "    # Check if radius has any non-NaN values before calling max()\n",
        "    if torch.isnan(radius).all():\n",
        "        print(f\"Warning: All radii are NaN for sigma={sigma}. Skipping plotting.\")\n",
        "        continue  # Skip plotting if all radii are NaN\n",
        "    x_radius, y_acc = calculate_certified_acc(radius, yp, y)\n",
        "    plt.plot(x_radius, y_acc, label=f\"sigma={sigma}\", color=color, linewidth=1)\n",
        "\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "wnWS3reROgu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_certified_acc(model_base, X, y)"
      ],
      "metadata": {
        "id": "4cWO-U4Fg6mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_certified_acc(model_base, X, y)"
      ],
      "metadata": {
        "id": "Z0ikxyjng7Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regular Training"
      ],
      "metadata": {
        "id": "26k9eu-vmZtk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyCAOqCdy-Gs"
      },
      "outputs": [],
      "source": [
        "\"\"\" Regular Training \"\"\"\n",
        "import json\n",
        "from torch.nn import CrossEntropyLoss, Conv2d\n",
        "\n",
        "model_reg = resnet18()\n",
        "\n",
        "# CIFAR10: kernel_size 7 -> 3, stride 2 -> 1, padding 3->1\n",
        "model_reg.conv1 = Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_reg.fc = torch.nn.Linear(model_reg.fc.in_features, num_classes)\n",
        "\n",
        "pretrained_weights = torch.load(weight_file, weights_only=True)\n",
        "model_reg.load_state_dict(pretrained_weights)\n",
        "model_reg = model_reg.to(device)\n",
        "\n",
        "run_dir = project_dir / generate_run_name()\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# opt = SGD(model_reg.parameters(), lr=1e-1)\n",
        "opt = Adam(model_reg.parameters(), lr=1e-3)\n",
        "\n",
        "epochs = 2\n",
        "log = {key: [] for key in [\"train_losses\", \"test_losses\", \"adv_losses\",\n",
        "                           \"train_errors\", \"test_errors\", \"adv_errors\"]}\n",
        "\n",
        "print(f\"Begin adversarial training run: {run_dir.stem}\\n\")\n",
        "print(*(\"TR      \", \"TE      \", \"ADV     \", \"     \"), sep=\"\\t\")\n",
        "\n",
        "for t in range(epochs):\n",
        "    train_err, train_loss = train_epoch(dataloader_train, model_reg, opt)\n",
        "    test_err, test_loss = eval_epoch(dataloader_test, model_reg)\n",
        "    adv_err, adv_loss = eval_epoch_adversarial(dataloader_test, model_reg, fgsm)\n",
        "\n",
        "    # Update the losses and errors\n",
        "    log[\"train_losses\"] += [train_loss]\n",
        "    log[\"test_losses\"] += [test_loss]\n",
        "    log[\"adv_losses\"] += [adv_loss]\n",
        "    log[\"train_errors\"] += [train_err]\n",
        "    log[\"test_errors\"] += [test_err]\n",
        "    log[\"adv_errors\"] += [adv_err]\n",
        "\n",
        "    print(*(\"{:.6f}\".format(train_err),\n",
        "            \"{:.6f}\".format(test_err),\n",
        "            \"{:.6f}\".format(adv_err),\n",
        "            f\"Epoch: {t+1}\",), sep=\"\\t\")\n",
        "\n",
        "with open(run_dir / \"log.json\", \"w\") as f:\n",
        "    json.dump(log, f)\n",
        "torch.save(model_reg.state_dict(), run_dir / \"model_reg.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ADV evaluated with FGSM\n",
        "We observe that the ADV validation error is lower than the training error because FGSM is a weaker attack thatn PGD\n",
        "Begin adversarial training run: magenta-slow-phoenix-20250105-1507\n",
        "\n",
        "TR      \tTE      \tADV     \tEpoch\n",
        "0.666760\t0.406778\t0.493000\t1\n",
        "0.539120\t0.314444\t0.405000\t2\n",
        "0.476300\t0.274778\t0.379000\t3\n",
        "0.432680\t0.245222\t0.339556\t4\n",
        "0.402620\t0.230444\t0.330000\t5\n",
        "\n",
        "ADV evaluated with PGD-Linf\n",
        "We observe that the ADV validation error is higher than the training error as expected\n",
        "Begin adversarial training run: orange-kind-shark-20250105-1531\n",
        "\n",
        "TR      \tTE      \tADV     \tEpoch\n",
        "0.675220\t0.419667\t0.599000\t1\n",
        "0.543500\t0.310889\t0.508778\t2\n",
        "0.472780\t0.271000\t0.475556\t3\n",
        "0.434380\t0.248000\t0.461889\t4\n",
        "0.403640\t0.230444\t0.457889\t5\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5tjaHMxCFwWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcUht5uBHlYG"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "gold-bright-eagle-20241204-1128:\n",
        "SGD(lr=1e-1), batch_size=1024\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "orange-kind-hawk-20241204-1148:\n",
        "SGD(lr=1e-1, weight_decay=5e-4), batch_size=1024\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "silver-shiny-phoenix-20241204-1210:\n",
        "SGD(lr=1e-1, weight_decay=5e-4, momentum=0.9, nesterov=True), batch_size=1024\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "gray-fierce-octopus-20241204-1226:\n",
        "SGD(lr=1e-1, weight_decay=5e-4, momentum=0.9, nesterov=True), batch_size=1024, CosineAnnealingLR\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "gold-loud-cheetah-20241204-1414:\n",
        "SGD(lr=1e-1, weight_decay=5e-4, momentum=0.9, nesterov=True), batch_size=1024,, epsilon=4/255\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "cyan-wise-eagle-20241203-1710:\n",
        "SGD(lr=1e-1, weight_decay=5e-4, momentum=0.9, nesterov=True), batch_size=1024,, epsilon=32/255\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-ys2JFSidFI5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}